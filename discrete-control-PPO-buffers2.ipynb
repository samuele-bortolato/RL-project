{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir('../input/rl-project')\n",
    "# import sys\n",
    "# sys.path.insert(0,'../input/rl-project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from agents import Agent\n",
    "from environment import SimulationEnvironment0\n",
    "from replay_buffers import *\n",
    "from utils import *\n",
    "\n",
    "import copy\n",
    "experiment_name='discrete_control_PPO_buffers_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 131072/131072 [1:02:21<00:00, 35.03it/s, 1.22      0.000595  0         ]\n",
      "100%|██████████| 131072/131072 [1:22:02<00:00, 26.62it/s, 1.19      -3.41e-05 0.0156    ]\n",
      "100%|██████████| 65536/65536 [1:04:35<00:00, 16.91it/s, 1.06      0.000465  0.0234    ]\n",
      "100%|██████████| 65536/65536 [1:57:41<00:00,  9.28it/s, 0.907     0.000508  0.00781   ]  \n",
      "100%|██████████| 32768/32768 [1:47:36<00:00,  5.07it/s, 0.764     -0.000902 -0.00781  ]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "EXPERIMENTS = [ {\"entropy\": 3e-4, 'epochs':1, 'training_steps':2**17, 'size': 2**16},\n",
    "                {\"entropy\": 3e-4, 'epochs':2, 'training_steps':2**17, 'size': 2**16},\n",
    "                {\"entropy\": 3e-4, 'epochs':4, 'training_steps':2**16, 'size': 2**16},\n",
    "                {\"entropy\": 3e-4, 'epochs':8, 'training_steps':2**16, 'size': 2**16},\n",
    "                {\"entropy\": 3e-4, 'epochs':16, 'training_steps':2**15, 'size': 2**16},\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# simulation\n",
    "num_simulations = 128\n",
    "num_blackholes = 1\n",
    "\n",
    "# agent\n",
    "hidden_size = 512\n",
    "simlog_res = 255\n",
    "use_symlog = True\n",
    "simlog_half_res = simlog_res//2\n",
    "simlog_max_range = 1\n",
    "actions_res = 5\n",
    "levels=2\n",
    "input_type = 'complete'\n",
    "\n",
    "lr = 3e-4\n",
    "lr_actor = 3e-5\n",
    "\n",
    "\n",
    "# training\n",
    "#training_steps = 2**18\n",
    "#epochs=8\n",
    "gamma = 0.98\n",
    "smoothing = 1e-2\n",
    "eps = 0.1\n",
    "\n",
    "batch_size = 2**10\n",
    "\n",
    "# replay buffers\n",
    "buffer = Replay_Buffer\n",
    "#size = 2**16\n",
    "batch_size = 2**10\n",
    "\n",
    "plot = False\n",
    "\n",
    "validate_every = 2**7\n",
    "\n",
    "bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n",
    "bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n",
    "\n",
    "dec_x, dec_y = torch.meshgrid(torch.arange(actions_res)/(actions_res-1)*2-1, torch.arange(actions_res)/(actions_res-1)*2-1)\n",
    "dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n",
    "\n",
    "metric_idx = torch.pow(2,torch.arange(15))-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "\n",
    "for experiment in EXPERIMENTS:\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",experiment_name, current_time + \"_\" + socket.gethostname() \n",
    "    )\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    sim = SimulationEnvironment0(num_simulations=128,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            max_steps=250,\n",
    "                            device='cuda')\n",
    "\n",
    "    if use_symlog:\n",
    "        actor = Agent((num_blackholes+2), hidden_size, levels, input_type, critic=False, action_dimension=actions_res**2).cuda()\n",
    "        critic = Agent((num_blackholes+2), hidden_size, levels, input_type, actor=False, value_dimension=simlog_res).cuda()\n",
    "    else:\n",
    "        actor = Agent((num_blackholes+2), hidden_size, levels, input_type, critic=False, action_dimension=actions_res**2).cuda()\n",
    "        critic = Agent((num_blackholes+2), hidden_size, levels, input_type, actor=False, value_dimension=1).cuda()\n",
    "\n",
    "    optim_actor = torch.optim.AdamW(actor.parameters(), lr=lr_actor, weight_decay=1e-3)\n",
    "    optim_critic = torch.optim.AdamW(critic.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    target_critic = copy.deepcopy(critic)\n",
    "    \n",
    "    replay_buffer = buffer(state_shape=((num_blackholes+2),2), action_shape=(1,), batch_size=batch_size, size=experiment['size'], device='cuda')\n",
    "\n",
    "    old_states=None\n",
    "\n",
    "    R=[]\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    pbar = tqdm(range(experiment['training_steps']))\n",
    "\n",
    "    x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n",
    "    pos = torch.stack([x.flatten(), y.flatten()],1)/100\n",
    "    target_pos = torch.ones_like(pos)*0.25\n",
    "    bh_pos = torch.ones_like(pos)*0.75\n",
    "\n",
    "    st=torch.stack([pos,target_pos,bh_pos],1)\n",
    "\n",
    "    E = []\n",
    "    plotV = []\n",
    "    plotVT = []\n",
    "    plotPolicy = []\n",
    "\n",
    "    \n",
    "    states = sim.get_state()\n",
    "    for i in pbar:\n",
    "        t0=time.time()\n",
    "\n",
    "        # generate experience\n",
    "        with torch.inference_mode():\n",
    "            # generate experience\n",
    "            states = states.reshape(states.shape[0],-1).cuda()\n",
    "            actions, _ = actor(states)\n",
    "\n",
    "            action_probs = actions.softmax(-1)\n",
    "            dist = Categorical(action_probs)\n",
    "            sampled_action = dist.sample()\n",
    "            prob = dist.log_prob(sampled_action).exp()\n",
    "\n",
    "            u, v = dec_x[sampled_action], dec_y[sampled_action]\n",
    "            sampled_action_decoded = torch.stack([u,v],1)\n",
    "\n",
    "            rewards, new_states, is_terminal = sim.step(sampled_action_decoded)\n",
    "            \n",
    "        replay_buffer.add_experience(states=states, actions=sampled_action, action_probs=prob, rewards=rewards, next_states=new_states, terminals=is_terminal)\n",
    "\n",
    "\n",
    "        R.append(rewards.mean().item())\n",
    "        tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n",
    "\n",
    "\n",
    "        # train critic on old experience\n",
    "        for b_idx in range(experiment['epochs']):\n",
    "            b_states, b_action, b_prob, b_rewards, b_next_states, b_terminals, _ = replay_buffer.get_batch()\n",
    "\n",
    "            _, values = critic(b_states.reshape(b_states.shape[0],-1))\n",
    "            actions, _ = actor(b_states.reshape(b_states.shape[0],-1))\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                _, values_t = target_critic(b_next_states.reshape(b_next_states.shape[0],-1).cuda())\n",
    "\n",
    "                expexted_next_value = (torch.softmax(values_t,1)@bin_values[:,None])[:,0]*(b_terminals.logical_not())\n",
    "\n",
    "                expecter_target_value = b_rewards + gamma*expexted_next_value\n",
    "\n",
    "            expexted_value = (torch.softmax(values,1)@bin_values[:,None])[:,0]\n",
    "            advantage = expecter_target_value - expexted_value\n",
    "\n",
    "            action_probs = actions.softmax(-1)\n",
    "            H = -(action_probs*(action_probs+1e-8).log()).sum(-1)\n",
    "            dist = Categorical(action_probs)\n",
    "            probs = dist.log_prob(b_action).exp()\n",
    "\n",
    "            r = probs/b_prob\n",
    "            L = torch.minimum(advantage*r, advantage*r.clip(1-eps,1+eps))\n",
    "\n",
    "            actor_error = -L -H*experiment['entropy']\n",
    "\n",
    "            y = two_hot_encode(expecter_target_value, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "            critic_error = (torch.nn.functional.cross_entropy(values, y, reduction='none'))*r\n",
    "\n",
    "            optim_actor.zero_grad()\n",
    "            actor_error.mean().backward()\n",
    "            optim_actor.step()\n",
    "\n",
    "            optim_critic.zero_grad()\n",
    "            critic_error.mean().backward()\n",
    "            optim_critic.step()\n",
    "\n",
    "        update_target_model(model=critic, target_model=target_critic, decay=1e-3)\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if i % 8 == 0:\n",
    "            pbar.set_postfix_str(f'{critic_error.mean().item():.3g}'.ljust(10)+\n",
    "                                 f'{actor_error.mean().item():.3g}'.ljust(10)+\n",
    "                                f'{rewards.mean().item():.3g}'.ljust(10))\n",
    "\n",
    "\n",
    "        if (i+1) % validate_every == 0:\n",
    "\n",
    "            V = []\n",
    "            A = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v = critic(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                a, _ = actor(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                V.append(v)\n",
    "                A.append(a)\n",
    "            V = torch.concat(V,0)\n",
    "            A = torch.concat(A,0)\n",
    "\n",
    "            V_t = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "\n",
    "                _, v = target_critic(stb.reshape(stb.shape[0],-1).cuda())\n",
    "\n",
    "                V_t.append(v)\n",
    "            V_t = torch.concat(V_t,0)\n",
    "\n",
    "\n",
    "            if use_symlog:\n",
    "                V = (V.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "                V_t = (V_t.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "            else:\n",
    "                V = V.cpu()\n",
    "                V_t = V_t.cpu()\n",
    "\n",
    "            tb_writer.add_image('V', (V.reshape(1,100,100)/2+0.5), i)\n",
    "            tb_writer.add_image('V_t', V_t.reshape(1,100,100)/2+0.5, i)\n",
    "\n",
    "            plotV.append(V.reshape(1,100,100).detach().cpu())\n",
    "            plotVT.append(V_t.reshape(1,100,100).detach().cpu())\n",
    "\n",
    "            pos = st[:,0].reshape(100,100,-1)[::2,::2]\n",
    "            A = A.reshape(100,100,-1)[::2,::2]\n",
    "            # A = A/2\n",
    "            action_probs = A.softmax(-1)\n",
    "            u = action_probs@dec_x[:,None]\n",
    "            v = action_probs@dec_y[:,None]\n",
    "            A = torch.concat([u,v],-1)\n",
    "\n",
    "            plt.quiver(pos[...,1].flatten(), -pos[...,0].flatten(), A[...,1].tanh().detach().cpu().flatten(), -A[...,0].tanh().detach().cpu().flatten(), color='g',scale=50, headwidth=2)\n",
    "            ax.axis('off')\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.subplots_adjust(0,0,1,1,0,0)\n",
    "            fig.canvas.draw()\n",
    "            data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.clf()\n",
    "\n",
    "            tb_writer.add_image('Policy visualization', np.transpose(data,(2,0,1)) , i)\n",
    "            plotPolicy.append(np.transpose(data,(2,0,1)))\n",
    "\n",
    "    # experiment.update({'E':torch.tensor(E), 'plotV':torch.stack(plotV), 'plotVT':torch.stack(plotVT), 'plotPolicy':torch.tensor(plotPolicy)})\n",
    "    torch.save(experiment,\n",
    "        os.path.join(log_dir,'results.pth'))\n",
    "\n",
    "\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
