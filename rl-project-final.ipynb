{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:14:01.949973Z","iopub.status.busy":"2023-08-28T01:14:01.949683Z","iopub.status.idle":"2023-08-28T01:14:01.966964Z","shell.execute_reply":"2023-08-28T01:14:01.965980Z","shell.execute_reply.started":"2023-08-28T01:14:01.949947Z"},"trusted":true},"outputs":[],"source":["# import os\n","# os.listdir('../input/rl-project')\n","# import sys\n","# sys.path.insert(0,'../input/rl-project/')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:14:01.971232Z","iopub.status.busy":"2023-08-28T01:14:01.970668Z","iopub.status.idle":"2023-08-28T01:14:15.228124Z","shell.execute_reply":"2023-08-28T01:14:15.227108Z","shell.execute_reply.started":"2023-08-28T01:14:01.971194Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","from tqdm import tqdm\n","\n","from functools import partial\n","\n","from IPython.display import clear_output\n","\n","from torch.utils.tensorboard import SummaryWriter\n","import socket\n","from datetime import datetime\n","import os\n","\n","from agents import Agent\n","from environment import SimulationEnvironment0\n","from replay_buffers import *\n","from utils import *\n","\n","import copy"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-28T01:14:15.232382Z","iopub.status.busy":"2023-08-28T01:14:15.231328Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Sam\\miniconda3\\envs\\torch1\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","  1%|          | 767/131072 [00:08<23:04, 94.09it/s] \n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 286\u001b[0m\n\u001b[0;32m    283\u001b[0m         optim_critic\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    284\u001b[0m         optim_actor\u001b[39m.\u001b[39mstep()\n\u001b[1;32m--> 286\u001b[0m     update_models(decay\u001b[39m=\u001b[39;49m\u001b[39m5e-3\u001b[39;49m)\n\u001b[0;32m    288\u001b[0m tb_writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mcritic_error_V\u001b[39m\u001b[39m'\u001b[39m,critic_error_V\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem(), i)\n\u001b[0;32m    289\u001b[0m tb_writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mcritic_error_Q\u001b[39m\u001b[39m'\u001b[39m,critic_error_Q\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\u001b[39m-\u001b[39mcritic_error_V\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem(), i)\n","File \u001b[1;32md:\\Scuola\\02_UniBo\\2_anno\\RL\\space\\utils.py:87\u001b[0m, in \u001b[0;36mupdate_target_models\u001b[1;34m(actor, V, Q, target_actor, target_V, target_Q, decay)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_target_models\u001b[39m(actor, V, Q, target_actor, target_V, target_Q, decay\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m):\n\u001b[0;32m     86\u001b[0m     update_target_model(model\u001b[39m=\u001b[39mactor, target_model\u001b[39m=\u001b[39mtarget_actor,decay\u001b[39m=\u001b[39mdecay)\n\u001b[1;32m---> 87\u001b[0m     update_target_model(model\u001b[39m=\u001b[39;49mV, target_model\u001b[39m=\u001b[39;49mtarget_V, decay\u001b[39m=\u001b[39;49mdecay)\n\u001b[0;32m     88\u001b[0m     update_target_model(model\u001b[39m=\u001b[39mQ, target_model\u001b[39m=\u001b[39mtarget_Q, decay\u001b[39m=\u001b[39mdecay)\n","File \u001b[1;32md:\\Scuola\\02_UniBo\\2_anno\\RL\\space\\utils.py:66\u001b[0m, in \u001b[0;36mupdate_target_model\u001b[1;34m(model, target_model, decay)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m weight_key, target_weight_key \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(model_dict\u001b[39m.\u001b[39mkeys(),target_model_dict\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m     65\u001b[0m     target_model_dict[target_weight_key] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mdecay)\u001b[39m*\u001b[39mtarget_model_dict[target_weight_key] \u001b[39m+\u001b[39m decay\u001b[39m*\u001b[39mmodel_dict[weight_key]\n\u001b[1;32m---> 66\u001b[0m target_model\u001b[39m.\u001b[39;49mload_state_dict(target_model_dict)\n","File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1657\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         out \u001b[39m=\u001b[39m hook(module, incompatible_keys)\n\u001b[0;32m   1651\u001b[0m         \u001b[39massert\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m, (\n\u001b[0;32m   1652\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1653\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1654\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mit should be done inplace.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1655\u001b[0m         )\n\u001b[1;32m-> 1657\u001b[0m load(\u001b[39mself\u001b[39;49m, state_dict)\n\u001b[0;32m   1658\u001b[0m \u001b[39mdel\u001b[39;00m load\n\u001b[0;32m   1660\u001b[0m \u001b[39mif\u001b[39;00m strict:\n","File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1645\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         child_prefix \u001b[39m=\u001b[39m prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1644\u001b[0m         child_state_dict \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m local_state_dict\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k\u001b[39m.\u001b[39mstartswith(child_prefix)}\n\u001b[1;32m-> 1645\u001b[0m         load(child, child_state_dict, child_prefix)\n\u001b[0;32m   1647\u001b[0m \u001b[39m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[0;32m   1648\u001b[0m incompatible_keys \u001b[39m=\u001b[39m _IncompatibleKeys(missing_keys, unexpected_keys)\n","File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1639\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[1;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[0;32m   1637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(module, local_state_dict, prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1638\u001b[0m     local_metadata \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m metadata\u001b[39m.\u001b[39mget(prefix[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], {})\n\u001b[1;32m-> 1639\u001b[0m     module\u001b[39m.\u001b[39;49m_load_from_state_dict(\n\u001b[0;32m   1640\u001b[0m         local_state_dict, prefix, local_metadata, \u001b[39mTrue\u001b[39;49;00m, missing_keys, unexpected_keys, error_msgs)\n\u001b[0;32m   1641\u001b[0m     \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1642\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1572\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1571\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m-> 1572\u001b[0m         param\u001b[39m.\u001b[39;49mcopy_(input_param)\n\u001b[0;32m   1573\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m   1574\u001b[0m     error_msgs\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mWhile copying the parameter named \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1575\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the model are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1576\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mwhose dimensions in the checkpoint are \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1577\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39man exception occurred : \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1578\u001b[0m                       \u001b[39m.\u001b[39mformat(key, param\u001b[39m.\u001b[39msize(), input_param\u001b[39m.\u001b[39msize(), ex\u001b[39m.\u001b[39margs))\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]},{"data":{"text/plain":["<Figure size 1000x1000 with 0 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["seed = 0\n","\n","EXPERIMENTS = [\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'ent', 'input_type':'complete', 'name':'discrete_ent', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'ent', 'input_type':'complete', 'name':'discrete_ent', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'ent', 'input_type':'complete', 'name':'discrete_ent', 'run':'ppo8' },\n","\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'discrete_prob', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'discrete_prob', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':True, 'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'discrete_prob', 'run':'ppo8' },\n","\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'continuous_prob', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'continuous_prob', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'continuous_prob', 'run':'ppo8' },\n","\n","\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':True, 'reg':'prob','input_type':'complete', 'name':'two_hot', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':True, 'reg':'prob','input_type':'complete', 'name':'two_hot', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':True, 'reg':'prob','input_type':'complete', 'name':'two_hot', 'run':'ppo8' },\n","\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'augmented','name':'input_aug', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'augmented','name':'input_aug', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'augmented','name':'input_aug', 'run':'ppo8' },\n","\n","                {'PPO':2, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'base',     'name':'input_base', 'run':'ppo2' },\n","                {'PPO':4, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'base',     'name':'input_base', 'run':'ppo4' },\n","                {'PPO':8, 'replay_ratio':0, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'base',     'name':'input_base', 'run':'ppo8' },\n","\n","\n","                # {'PPO':2, 'replay_ratio':1, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo2_rr1' },\n","                # {'PPO':2, 'replay_ratio':4, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo2_rr4' },\n","                # {'PPO':2, 'replay_ratio':8, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo2_rr8' },\n","\n","                # {'PPO':4, 'replay_ratio':1, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo4_rr1' },\n","                # {'PPO':4, 'replay_ratio':4, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo4_rr4' },\n","                # {'PPO':4, 'replay_ratio':8, 'discrete_actions':False,'use_two_hot':False,'reg':'prob','input_type':'complete', 'name':'ppo_acer_prob', 'run':'ppo4_rr8' },\n","\n","                # {'PPO':2, 'replay_ratio':1, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo2_rr1' },\n","                # {'PPO':2, 'replay_ratio':4, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo2_rr4' },\n","                # {'PPO':2, 'replay_ratio':8, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo2_rr8' },\n","\n","                # {'PPO':4, 'replay_ratio':1, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo4_rr1' },\n","                # {'PPO':4, 'replay_ratio':4, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo4_rr4' },\n","                # {'PPO':4, 'replay_ratio':8, 'discrete_actions':False,'use_two_hot':False,'reg':'sac', 'input_type':'complete', 'name':'ppo_acer_sac', 'run':'ppo4_rr8' },\n","\n","               ]\n","\n","\n","# simulation\n","num_simulations = 128\n","num_blackholes = 1\n","\n","# agent\n","hidden_size = 512\n","simlog_res = 255\n","simlog_half_res = simlog_res//2\n","simlog_max_range = 1\n","actions_res = 5\n","levels=2\n","\n","lr = 3e-4\n","lr_actor = 3e-5\n","\n","# training\n","training_steps=2**17\n","lamb = 0.8\n","gamma = 0.98\n","smoothing = 1e-2\n","eps = 0.05 # for PPO update\n","seg_len = 2**5\n","c=1\n","\n","entropy = 3e-4\n","\n","# replay buffers\n","num_steps = 1024\n","batch_size = 2**10\n","replay_batch_size = num_simulations\n","\n","plot = False\n","\n","validate_every = 2**9\n","\n","bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n","bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n","\n","dec_x, dec_y = torch.meshgrid(torch.arange(actions_res)/(actions_res-1)*2-1, torch.arange(actions_res)/(actions_res-1)*2-1)\n","dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n","\n","metric_idx = torch.pow(2,torch.arange(15))-1\n","\n","fig, ax = plt.subplots(figsize=(10,10))\n","\n","for experiment in EXPERIMENTS:\n","    \n","    try:\n","        # SETUP EXPERIMENT\n","\n","        # set seed\n","        torch.manual_seed(seed)\n","\n","        # initialize logging (with tensorboard)\n","        current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n","        log_dir = os.path.join(\n","            \"runs\",experiment['name'], experiment['run'], str(seed), current_time + \"_\" + socket.gethostname() \n","        )\n","        tb_writer = SummaryWriter(log_dir)\n","\n","        # initialize the simulation\n","        sim = SimulationEnvironment0(num_simulations=num_simulations,\n","                                num_blackholes=num_blackholes, \n","                                force_constant=0.002, \n","                                velocity_scale=0.01,\n","                                goal_threshold=0.05,\n","                                max_steps=250,\n","                                device='cuda')\n","        states = sim.get_state()\n","\n","        # initialize the networks\n","        if experiment['discrete_actions']:\n","            action_dim = actions_res**2             # output shape of the actor\n","            compute_dist = compute_dist_discrete\n","            decode_action = partial(decode_action_discrete, dec_x=dec_x, dec_y=dec_y)\n","            action_shape = ()                     # shape after sampling the dist, before decoding\n","        else:\n","            action_dim = 5\n","            compute_dist = compute_dist_continuous\n","            decode_action = decode_action_continuous\n","            action_shape = (2,)\n","\n","        if experiment['use_two_hot']:\n","            value_dimension = simlog_res\n","            decode_values = partial(decode_value_symlog, bin_values)\n","            critic_error_func = partial(critic_error_func_symlog, simlog_max_range, simlog_res, simlog_half_res, smoothing)\n","        else:\n","            value_dimension = 1\n","            decode_values = lambda x : x[:,0]\n","            critic_error_func = critic_error_func_normal\n","\n","        actor = Agent((num_blackholes+2)*2, hidden_size, levels, experiment['input_type'], critic=False, action_dimension=action_dim).cuda()\n","        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, experiment['input_type'], critic=False, action_dimension=action_dim).cuda()\n","        V = Agent((num_blackholes+2)*2, hidden_size, levels, experiment['input_type'], actor=False, value_dimension=value_dimension).cuda()\n","        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=value_dimension).cuda()\n","\n","\n","        optim_actor = torch.optim.AdamW(list(actor.parameters())+list(actor2.parameters()), lr=lr_actor, weight_decay=1e-3)\n","        optim_critic = torch.optim.AdamW(list(V.parameters())+list(Q.parameters()), lr=lr, weight_decay=1e-3)\n","        target_actor = copy.deepcopy(actor)\n","        target_V = copy.deepcopy(V)\n","        target_Q = copy.deepcopy(Q)\n","\n","        update_models = partial(update_target_models, actor, V, Q, target_actor, target_V, target_Q)\n","\n","        # initialize the replay buffer\n","        replay_buffer = Replay_Buffer_Segments(state_shape=((num_blackholes+2),2), \n","                                                action_shape=action_shape, \n","                                                params_shape=(action_dim,), \n","                                                segment_lenght=seg_len, \n","                                                num_simulations=num_simulations, \n","                                                num_steps=num_steps, \n","                                                batch_size=replay_batch_size, \n","                                                device='cuda')\n","\n","        # initialize vailidation plane\n","        x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n","        pos = torch.stack([x.flatten(), y.flatten()],1)/100\n","\n","        target_pos = torch.ones_like(pos)*0.25      # position of the target\n","        bh_pos = torch.ones_like(pos)*0.75          # position of the blackholes\n","\n","        st=torch.stack([pos,target_pos,bh_pos],1)\n","\n","        plots = partial(validation_plots, tb_writer, st, batch_size, V, target_V, actor, decode_values, experiment['discrete_actions'], dec_x, dec_y, fig, ax)\n","\n","\n","        # START EXPERIMENT\n","        pbar = tqdm(range(training_steps))\n","        for i in pbar:\n","            t0=time.time()\n","\n","            # GENERATE EXPERIENCE\n","            with torch.inference_mode():\n","\n","                states = states.reshape(states.shape[0],-1).cuda()\n","\n","                # compute action distribution according to the current policy\n","                actions, _, _ = actor(states)\n","                dist = compute_dist(actions)\n","\n","                # sample an action\n","                sampled_action = dist.sample()\n","                sampled_action_decoded, log_prob = decode_action(dist, sampled_action)\n","\n","                # simulation step\n","                rewards, new_states, terminals = sim.step(sampled_action_decoded)\n","\n","                # save experience\n","                replay_buffer.add_experience(states.reshape(new_states.shape), sampled_action, actions, log_prob, rewards, terminals)\n","\n","            tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n","\n","            if (i+1) % seg_len == 0:\n","\n","                # ON POLICY UPDATE\n","\n","                seg_state, seg_actions, _, seg_action_log_probs, seg_rewards, seg_terminal = replay_buffer.get_last_segment()\n","\n","                with torch.inference_mode():\n","\n","                    # get target values for all the states in the segment\n","                    V_t = torch.zeros(num_simulations*seg_len, device='cuda')\n","                    for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n","\n","                        b_state = seg_state.reshape((seg_len)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n","\n","                        _, Vs, _= target_V(b_state)\n","                        V_t[b_idx*batch_size:(b_idx+1)*batch_size] = decode_values(Vs)\n","\n","                    V_t = V_t.reshape(num_simulations, seg_len)\n","\n","                    # compute GAE and TD lambda returns\n","                    gae = torch.zeros_like(V_t)\n","                    for t in reversed(range(seg_len-1)):\n","\n","                        d_t = -V_t[:,t] + seg_rewards[:,t] + gamma*V_t[:,t+1]*seg_terminal[:,t].logical_not()\n","\n","                        if experiment['reg']=='sac':\n","                            d_t = d_t - seg_action_log_probs[:,t]*entropy\n","\n","                        gae[:,t] = d_t + gamma*lamb*gae[:,t+1]*seg_terminal[:,t].logical_not()\n","\n","                    tdl = V_t + gae\n","\n","                # unite batch and sequence dimensions\n","                [gae, tdl, seg_state, seg_actions, seg_action_log_probs] = flatten_sequences([gae, tdl, seg_state, seg_actions, seg_action_log_probs], removelast=True)\n","\n","                # run PPO for n epochs\n","                for ppo_epoch in range(experiment['PPO']):\n","                    for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n","\n","                        # get batch\n","                        [b_gae, b_tdl, b_state, b_action, b_log_prob] = get_batch([gae, tdl, seg_state, seg_actions, seg_action_log_probs], batch_size, b_idx)\n","\n","                        # compute action distribution according to the updated policy\n","                        actions, _, _ = actor(b_state)\n","                        dist = compute_dist(actions)\n","\n","                        # compute probability of the chosen action with respect to the updated policy\n","                        sampled_action_decoded, log_prob = decode_action(dist, b_action)\n","\n","                        # compute the value functions\n","                        _, Vs, hs = V(b_state)\n","                        _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n","\n","                        # compute PPO clip objective\n","                        r = (log_prob.exp()+1e-5) / (b_log_prob.exp()+1e-5)\n","                        L = torch.minimum(b_gae*r, b_gae*r.clip(1-eps,1+eps))\n","\n","                        # approximate entropy\n","                        H = - log_prob\n","\n","                        actor_error = - L + 1e-5*actions.square().mean()\n","\n","                        if experiment['reg']=='ent':\n","                            if experiment['discrete_actions']:\n","                                action_probs = actions.softmax(-1)\n","                                h = (action_probs * (action_probs+1e-8).log()).sum(-1)\n","                                actor_error = actor_error + h*entropy\n","                            else:\n","                                actor_error = actor_error + log_prob*entropy\n","                        elif experiment['reg']=='prob':\n","                            actor_error = actor_error + log_prob.exp()*entropy\n","\n","                        critic_error_V = critic_error_func(Vs, b_tdl) \n","                        critic_error_Q = critic_error_func(Vs+Qs, b_tdl)\n","                        critic_error = (critic_error_V + critic_error_Q) / 2\n","                        error = actor_error + critic_error\n","\n","                        optim_critic.zero_grad()\n","                        optim_actor.zero_grad() \n","                        error.mean().backward()\n","                        optim_critic.step()\n","                        optim_actor.step()\n","\n","                    update_models(decay=5e-3)\n","\n","                tb_writer.add_scalar('critic_error_V',critic_error_V.mean().item(), i)\n","                tb_writer.add_scalar('critic_error_Q',critic_error_Q.mean().item()-critic_error_V.mean().item(), i)\n","                tb_writer.add_scalar('hentropy',H.mean().item(), i)\n","\n","                # REPLAY EXPERIENCES\n","\n","                # replay experiences\n","                for replay_epoch in range(experiment['replay_ratio']):\n","\n","                    seg_state, seg_actions, seg_action_params, seg_action_log_probs, seg_rewards, seg_terminal = replay_buffer.get_batch()\n","\n","                    with torch.inference_mode():\n","\n","                        # get target values and action probs for all the states in the segment\n","\n","                        V_t, Q_t, log_probs_t, Q_t_corr, log_probs_corr, log_probs_seg_corr = initialize_zeros(replay_batch_size*seg_len, n=6, device='cuda')\n","                        action_corr = torch.zeros((replay_batch_size*seg_len,) + action_shape, device='cuda')\n","\n","                        for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n","\n","                            # get batch\n","                            [b_state, b_action, b_seg_action_params] = get_batch(flatten_sequences([seg_state, seg_actions, seg_action_params]), batch_size, b_idx)\n","\n","                            # compute action distribution according to current policy\n","                            actions, _, _ = target_actor(b_state)\n","                            dist = compute_dist(actions)\n","\n","                            # compute the probability of the replayed action\n","                            sampled_action_decoded, log_prob = decode_action(dist, b_action)\n","\n","                            # sample a new action from the current policy (for the bias correction)\n","                            sampled_action_corr = dist.sample()\n","                            sampled_action_decoded_corr, log_prob_corr = decode_action(dist, sampled_action_corr)\n","\n","                            # compute the probabilty of the action sampled with the current policy with respect to the old policy (for the ro in bias correction)\n","                            seg_dist = compute_dist(b_seg_action_params)\n","                            _, seg_log_prob_corr = decode_action(seg_dist, sampled_action_corr)\n","\n","                            # compute the value functions\n","                            _, Vs, hs = target_V(b_state)\n","                            _, Qs, _ = target_Q(torch.concat([hs, sampled_action_decoded],-1))\n","                            _, Qs_corr, _ = target_Q(torch.concat([hs, sampled_action_decoded_corr],-1))\n","\n","                            b_V_t = decode_values(Vs)\n","                            b_Q_t = decode_values(Vs+Qs)\n","                            b_Q_t_corr = decode_values(Vs+Qs_corr)\n","\n","                            update_batched( [V_t, Q_t, log_probs_t, Q_t_corr, action_corr, log_probs_corr, log_probs_seg_corr],\n","                                            [b_V_t, b_Q_t, log_prob, b_Q_t_corr, sampled_action_corr, log_prob_corr, seg_log_prob_corr],\n","                                            batch_size, b_idx)\n","\n","                        # reshape to expose the sequences\n","                        [V_t, Q_t, log_probs_t, Q_t_corr, action_corr, log_probs_corr, log_probs_seg_corr] = reshape_sequences([V_t, Q_t, log_probs_t, Q_t_corr, action_corr, log_probs_corr, log_probs_seg_corr], (replay_batch_size, seg_len))\n","\n","\n","                        # compute targets (as in RETRACE)\n","                        Q_ret = torch.zeros_like(V_t)\n","                        V_target = torch.zeros_like(V_t)\n","\n","                        Q_ret[:,-1] = Q_t[:,-1]\n","\n","                        for t in reversed(range(seg_len-1)):\n","\n","                            ro = (log_probs_t[:,t+1].exp()+1e-5)/(seg_action_log_probs[:,t+1].exp()+1e-5) # ro of t+1\n","                            ci = lamb * torch.minimum(torch.ones(1, device='cuda'), ro)\n","\n","                            Q_ret[:,t] = seg_rewards[:,t] + gamma*(ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1])\n","\n","                            if experiment['reg']=='sac':\n","                                Q_ret[:,t] = Q_ret[:,t] - log_probs_t[:,t]*entropy\n","\n","                            V_target[:, t+1] = ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1]\n","\n","                        V_target[:, 0] = ci*(Q_ret[:,0] - Q_t[:,0]) + Q_t_corr[:,0]\n","\n","                    # unite batch and sequence dimensions\n","                    [Q_ret, V_target, seg_state, seg_actions, log_probs_t, seg_action_log_probs] = flatten_sequences([Q_ret, V_target, seg_state, seg_actions, log_probs_t, seg_action_log_probs], removelast=True)\n","                    [Q_t_corr, V_t, action_corr, log_probs_corr, log_probs_seg_corr] = flatten_sequences([Q_t_corr, V_t, action_corr, log_probs_corr, log_probs_seg_corr], removelast=True)\n","\n","                    # run PPO for n epochs\n","                    for ppo_epoch in range(experiment['PPO']):\n","                        for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n","\n","                            # get batch\n","                            [b_Q_ret, b_V_target, b_state, b_action, b_log_prob, b_log_prob_seg] = get_batch([Q_ret, V_target, seg_state, seg_actions, log_probs_t, seg_action_log_probs], batch_size, b_idx)\n","                            [b_Q_t_corr, b_V_t, b_action_corr, b_log_probs_corr, b_log_probs_seg_corr] = get_batch([Q_t_corr, V_t, action_corr, log_probs_corr, log_probs_seg_corr], batch_size, b_idx) # bias correction\n","\n","                            # compute action distribution according to current policy\n","                            actions, _, _ = actor(b_state)\n","                            dist = compute_dist(actions)\n","\n","                            # compute probability of the chosen action with respect to the updated policy\n","                            sampled_action_decoded, log_prob = decode_action(dist, b_action)\n","\n","                            # compute probability of the bias-correction action with respect to the updated policy\n","                            _, log_prob_corr = decode_action(dist, b_action_corr)\n","\n","                            # compute the value functions\n","                            _, Vs, hs = V(b_state)\n","                            _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n","\n","                            # compute the PPO objective (with respect to the target policy) and apply off policy correction (with bias correction)\n","                            r = (log_prob.exp()+1e-5)/(b_log_prob.exp()+1e-5)\n","                            adv = ((b_log_prob.exp()+1e-5)/(b_log_prob_seg.exp()+1e-5)).clip(max = c)*(b_Q_ret - b_V_target)\n","\n","                            r_c = (log_prob_corr.exp()+1e-5)/(b_log_probs_corr.exp()+1e-5)\n","                            adv_c = (1 - c*(b_log_probs_seg_corr.exp()+1e-5)/(b_log_probs_corr.exp()+1e-5)).relu()*(b_Q_t_corr - b_V_t)\n","\n","                            L = torch.minimum(adv*r, adv*r.clip(1-eps,1+eps)) + torch.minimum(adv_c*r_c, adv_c*r_c.clip(1-eps,1+eps))\n","\n","                            # approximate entropy\n","                            H = - log_prob\n","\n","                            actor_error = - L + 1e-5*actions.square().mean()\n","\n","                            if experiment['reg']=='ent':\n","                                if experiment['discrete_actions']:\n","                                    action_probs = actions.softmax(-1)\n","                                    h = (action_probs * (action_probs+1e-8).log()).sum(-1)\n","                                    actor_error = actor_error + h*entropy\n","                                else:\n","                                    actor_error = actor_error + log_prob*entropy\n","                            elif experiment['reg']=='prob':\n","                                actor_error = actor_error + log_prob.exp()*entropy\n","\n","                            critic_error_V = critic_error_func(Vs, b_V_target)\n","                            critic_error_Q = critic_error_func(Vs+Qs, b_Q_ret)\n","                            critic_error = (critic_error_V + critic_error_Q) / 2\n","                            error = actor_error + critic_error\n","\n","                            optim_critic.zero_grad()\n","                            optim_actor.zero_grad() \n","                            error.mean().backward()\n","                            optim_critic.step()\n","                            optim_actor.step()\n","\n","                        update_models(decay=5e-3)\n","\n","\n","            states = new_states\n","\n","            if (i+1) % validate_every == 0:\n","                plots(i)\n","                \n","    except Exception as error:\n","        print(\"An exception occurred:\", error)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
