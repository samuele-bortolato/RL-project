{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir('../input/rl-project')\n",
    "# import sys\n",
    "# sys.path.insert(0,'../input/rl-project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from agents import Agent\n",
    "from environment import SimulationEnvironment0\n",
    "from replay_buffers import *\n",
    "from utils import *\n",
    "\n",
    "import copy\n",
    "experiment_name='final_2_corr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1f797576760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_discrete(actions):\n",
    "    action_probs = actions.softmax(-1)\n",
    "    dist = Categorical(action_probs)\n",
    "    return dist\n",
    "\n",
    "def compute_dist_continuous(actions):\n",
    "    \n",
    "    means = actions[:,:2]\n",
    "    correlations = actions[:,2:-2].tanh()*(1-1e-5)\n",
    "    variances = actions[:,-2:].exp()\n",
    "\n",
    "    b,n = means.shape\n",
    "    corr_matrix = torch.zeros(b, n, n, device=means.device)\n",
    "    indices = torch.tril_indices(n, n, offset=-1)\n",
    "    corr_matrix[..., indices[0], indices[1]] = correlations\n",
    "    corr_matrix[..., indices[1], indices[0]] = correlations\n",
    "    corr_matrix[..., torch.arange(n), torch.arange(n)] = 1\n",
    "    cov_matrix = corr_matrix * variances[...,None] * variances[:,None]\n",
    "    \n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(means, cov_matrix)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action_discrete(dist, sampled_action):\n",
    "    # WARNING: uses global dec_x dec_y to decode the actions!\n",
    "    log_prob = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = torch.stack([dec_x[sampled_action],dec_y[sampled_action]],1)\n",
    "    return sampled_action_decoded, log_prob\n",
    "\n",
    "def decode_action_continuous(dist, sampled_action):\n",
    "    log_prob_sample = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = sampled_action.tanh()\n",
    "    log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1) # correct accounting for the tanh transform\n",
    "    return sampled_action_decoded, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_models(decay=1e-3):\n",
    "    update_target_model(model=actor, target_model=target_actor,decay=decay)\n",
    "    update_target_model(model=V, target_model=target_V, decay=decay)\n",
    "    update_target_model(model=Q, target_model=target_Q, decay=decay)\n",
    "\n",
    "def flatten_sequences(X, removelast=False):\n",
    "    for i in range(len(X)):\n",
    "        if removelast:\n",
    "            X[i] = X[i][:,:-1]\n",
    "        s = X[i].shape\n",
    "        if len(s)==2:\n",
    "            X[i] = X[i].reshape(s[0]*s[1])\n",
    "        else:\n",
    "            X[i] = X[i].reshape(s[0]*s[1],-1)\n",
    "    return X\n",
    "\n",
    "def reshape_sequences(X, shape):\n",
    "    for i in range(len(X)):\n",
    "        if X[i].shape.numel()==np.prod(shape):\n",
    "            X[i] = X[i].reshape(shape)\n",
    "        else:\n",
    "            X[i] = X[i].reshape(shape+(-1,))\n",
    "    return X\n",
    "\n",
    "def get_batch(X, batch_size, b_idx):\n",
    "    start = b_idx*batch_size\n",
    "    end = (b_idx+1)*batch_size\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i][start:end]\n",
    "    return X\n",
    "\n",
    "def initialize_zeros(shape, n, device):\n",
    "    X=[]\n",
    "    for _ in range(n):\n",
    "        X.append(torch.zeros(shape, device=device))\n",
    "    return X\n",
    "\n",
    "def update_batched(X, U, batch_size, b_idx):\n",
    "    start = b_idx*batch_size\n",
    "    end = (b_idx+1)*batch_size\n",
    "    for i in range(len(X)):\n",
    "        X[i][start:end] = U[i]\n",
    "    return X\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1290"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 71583/131072 [19:44<16:24, 60.41it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 193\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39m# compute action distribution according to the updated policy\u001b[39;00m\n\u001b[0;32m    192\u001b[0m actions, _, _ \u001b[39m=\u001b[39m actor(b_state)\n\u001b[1;32m--> 193\u001b[0m dist \u001b[39m=\u001b[39m compute_dist(actions)\n\u001b[0;32m    195\u001b[0m \u001b[39m# compute probability of the chosen action with respect to the updated policy\u001b[39;00m\n\u001b[0;32m    196\u001b[0m sampled_action_decoded, log_prob \u001b[39m=\u001b[39m decode_action(dist, b_action)\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mcompute_dist_continuous\u001b[1;34m(actions)\u001b[0m\n\u001b[0;32m     17\u001b[0m corr_matrix[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, torch\u001b[39m.\u001b[39marange(n), torch\u001b[39m.\u001b[39marange(n)] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     18\u001b[0m cov_matrix \u001b[39m=\u001b[39m corr_matrix \u001b[39m*\u001b[39m variances[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m variances[:,\u001b[39mNone\u001b[39;00m]\n\u001b[1;32m---> 20\u001b[0m dist \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mmultivariate_normal\u001b[39m.\u001b[39;49mMultivariateNormal(means, cov_matrix)\n\u001b[0;32m     22\u001b[0m \u001b[39mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:150\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m=\u001b[39m loc\u001b[39m.\u001b[39mexpand(batch_shape \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[0;32m    149\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 150\u001b[0m \u001b[39msuper\u001b[39;49m(MultivariateNormal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, event_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n\u001b[0;32m    152\u001b[0m \u001b[39mif\u001b[39;00m scale_tril \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\distributions\\distribution.py:54\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     53\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[1;32m---> 54\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39;49mcheck(value)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     57\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\distributions\\constraints.py:510\u001b[0m, in \u001b[0;36m_PositiveDefinite.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[0;32m    509\u001b[0m     sym_check \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sym_check\u001b[39m.\u001b[39mall():\n\u001b[0;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m sym_check\n\u001b[0;32m    512\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcholesky_ex(value)\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39meq(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "EXPERIMENTS = [#{\"entropy\": 3e-4, 'PPO':4, 'replay_ratio':0, 'training_steps':2**17},\n",
    "               {\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':1, 'training_steps':2**17},\n",
    "               #{\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':3, 'training_steps':2**17}\n",
    "               ]\n",
    "\n",
    "\n",
    "# simulation\n",
    "num_simulations = 128\n",
    "num_blackholes = 1\n",
    "\n",
    "# agent\n",
    "hidden_size = 512\n",
    "simlog_res = 255\n",
    "use_symlog = True\n",
    "discrete_actions = False\n",
    "simlog_half_res = simlog_res//2\n",
    "simlog_max_range = 1\n",
    "actions_res = 5\n",
    "levels=2\n",
    "input_type = 'complete'\n",
    "\n",
    "lr = 3e-4\n",
    "lr_actor = 3e-5\n",
    "\n",
    "\n",
    "\n",
    "# training\n",
    "#training_steps = 2**18\n",
    "#epochs=8\n",
    "lamb = 0.8\n",
    "gamma = 0.98\n",
    "smoothing = 1e-2\n",
    "eps = 0.05 # for PPO update\n",
    "seg_len = 2**5\n",
    "h_samples = 0\n",
    "c=0.1\n",
    "\n",
    "\n",
    "# replay buffers\n",
    "num_steps = 1024\n",
    "batch_size = 2**10\n",
    "replay_batch_size = num_simulations\n",
    "\n",
    "plot = False\n",
    "\n",
    "validate_every = 2**9\n",
    "\n",
    "bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n",
    "bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n",
    "\n",
    "#dec_x, dec_y = torch.meshgrid(torch.arange(actions_res)/(actions_res-1)*2-1, torch.arange(actions_res)/(actions_res-1)*2-1)\n",
    "#dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n",
    "\n",
    "metric_idx = torch.pow(2,torch.arange(15))-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "for experiment in EXPERIMENTS:\n",
    "\n",
    "    # SETUP EXPERIMENT\n",
    "\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # initialize logging (with tensorboard)\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",experiment_name, current_time + \"_\" + socket.gethostname() \n",
    "    )\n",
    "    tb_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # initialize the simulation\n",
    "    sim = SimulationEnvironment0(num_simulations=num_simulations,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            max_steps=250,\n",
    "                            device='cuda')\n",
    "    states = sim.get_state()\n",
    "\n",
    "    # initialize the networks\n",
    "    if discrete_actions:\n",
    "        action_dim = actions_res**2\n",
    "        compute_dist = compute_dist_discrete\n",
    "        decode_action = decode_action_discrete\n",
    "    else:\n",
    "        action_dim = 5\n",
    "        compute_dist = compute_dist_continuous\n",
    "        decode_action = decode_action_continuous\n",
    "\n",
    "    if use_symlog:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=simlog_res).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=simlog_res).cuda()\n",
    "    else:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=1).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=1).cuda()\n",
    "\n",
    "    optim_actor = torch.optim.AdamW(list(actor.parameters())+list(actor2.parameters()), lr=lr_actor, weight_decay=1e-3)\n",
    "    optim_critic = torch.optim.AdamW(list(V.parameters())+list(Q.parameters()), lr=lr, weight_decay=1e-3)\n",
    "    target_actor = copy.deepcopy(actor)\n",
    "    target_V = copy.deepcopy(V)\n",
    "    target_Q = copy.deepcopy(Q)\n",
    "\n",
    "    # initialize the replay buffer\n",
    "    replay_buffer = Replay_Buffer_Segments(state_shape=((num_blackholes+2),2), action_shape=(2,), params_shape=(action_dim,), segment_lenght=seg_len, num_simulations=num_simulations, num_steps=num_steps, batch_size=replay_batch_size, device='cuda')\n",
    "\n",
    "    # initialize vailidation plane\n",
    "    x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n",
    "    pos = torch.stack([x.flatten(), y.flatten()],1)/100\n",
    "\n",
    "    target_pos = torch.ones_like(pos)*0.25      # position of the target\n",
    "    bh_pos = torch.ones_like(pos)*0.75          # position of the blackholes\n",
    "\n",
    "    st=torch.stack([pos,target_pos,bh_pos],1)\n",
    "\n",
    "\n",
    "    # START EXPERIMENT\n",
    "    pbar = tqdm(range(experiment['training_steps']))\n",
    "    for i in pbar:\n",
    "        t0=time.time()\n",
    "\n",
    "        # GENERATE EXPERIENCE\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            states = states.reshape(states.shape[0],-1).cuda()\n",
    "\n",
    "            # compute action distribution according to the current policy\n",
    "            actions, _, _ = actor(states)\n",
    "            dist = compute_dist(actions)\n",
    "\n",
    "            # sample an action\n",
    "            sampled_action = dist.sample()\n",
    "            sampled_action_decoded, log_prob = decode_action(dist, sampled_action)\n",
    "\n",
    "            # simulation step\n",
    "            rewards, new_states, terminals = sim.step(sampled_action_decoded)\n",
    "\n",
    "            # save experience\n",
    "            replay_buffer.add_experience(states.reshape(new_states.shape), sampled_action, actions, log_prob.exp(), rewards, terminals)\n",
    "\n",
    "        tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n",
    "\n",
    "        if (i+1) % seg_len == 0:\n",
    "\n",
    "            # ON POLICY UPDATE\n",
    "\n",
    "            seg_state, seg_actions, _, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_last_segment()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "\n",
    "                # get target values for all the states in the segment\n",
    "                V_t = torch.zeros(num_simulations*seg_len, device='cuda')\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    b_state = seg_state.reshape((seg_len)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    \n",
    "                    _, Vs, _= target_V(b_state)\n",
    "                    V_t[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "        \n",
    "                V_t = V_t.reshape(num_simulations, seg_len)\n",
    "\n",
    "                # compute GAE and TD lambda returns\n",
    "                gae = torch.zeros_like(V_t)\n",
    "                for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                    d_t = -V_t[:,t] + seg_rewards[:,t] + gamma*V_t[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                    gae[:,t] = d_t + gamma*lamb*gae[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                tdl = V_t + gae\n",
    "\n",
    "            # unite batch and sequence dimensions\n",
    "            [gae, tdl, seg_state, seg_actions, seg_action_probs] = flatten_sequences([gae, tdl, seg_state, seg_actions, seg_action_probs], removelast=True)\n",
    "\n",
    "            # run PPO for n epochs\n",
    "            for ppo_epoch in range(experiment['PPO']):\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    # get batch\n",
    "                    [b_gae, b_tdl, b_state, b_action, b_prob] = get_batch([gae, tdl, seg_state, seg_actions, seg_action_probs], batch_size, b_idx)\n",
    "\n",
    "                    # compute action distribution according to the updated policy\n",
    "                    actions, _, _ = actor(b_state)\n",
    "                    dist = compute_dist(actions)\n",
    "\n",
    "                    # compute probability of the chosen action with respect to the updated policy\n",
    "                    sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                    # compute the value functions\n",
    "                    _, Vs, hs = V(b_state)\n",
    "                    _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "                    # compute PPO clip objective\n",
    "                    r = (log_prob.exp() + 1e-5)/(b_prob + 1e-5)\n",
    "                    L = torch.minimum(b_gae*r, b_gae*r.clip(1-eps,1+eps))\n",
    "\n",
    "                    # approximation of the hentropy\n",
    "                    H = -log_prob.exp()\n",
    "\n",
    "                    actor_error = - L - H*experiment['entropy']\n",
    "\n",
    "                    y = two_hot_encode(b_tdl, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                    critic_error_V = torch.nn.functional.cross_entropy(Vs, y, reduction='none')\n",
    "                    critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y, reduction='none')\n",
    "                    critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                    error = actor_error + critic_error\n",
    "\n",
    "                    optim_critic.zero_grad()\n",
    "                    optim_actor.zero_grad() \n",
    "                    error.mean().backward()\n",
    "                    optim_critic.step()\n",
    "                    optim_actor.step()\n",
    "                \n",
    "                update_models(decay=5e-3)\n",
    "\n",
    "            tb_writer.add_scalar('critic_error_V',critic_error_V.mean().item(), i)\n",
    "            tb_writer.add_scalar('critic_error_Q',critic_error_Q.mean().item()-critic_error_V.mean().item(), i)\n",
    "            tb_writer.add_scalar('hentropy',H.mean().item(), i)\n",
    "\n",
    "            # REPLAY EXPERIENCES\n",
    "\n",
    "            # replay experiences\n",
    "            for replay_epoch in range(experiment['replay_ratio']):\n",
    "\n",
    "                seg_state, seg_actions, seg_action_params, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_batch()\n",
    "\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    # get target values and action probs for all the states in the segment\n",
    "\n",
    "                    V_t, Q_t, probs_t, Q_t_corr, probs_corr, probs_seg_corr = initialize_zeros(replay_batch_size*seg_len, n=6, device='cuda')\n",
    "                    action_corr = torch.zeros(replay_batch_size*seg_len, 2, device='cuda')\n",
    "\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        # get batch\n",
    "                        [b_state, b_action, b_seg_action_params] = get_batch(flatten_sequences([seg_state, seg_actions, seg_action_params]), batch_size, b_idx)\n",
    "\n",
    "                        # compute action distribution according to current policy\n",
    "                        actions, _, _ = target_actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        # compute the probability of the replayed action\n",
    "                        sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                        # sample a new action from the current policy (for the bias correction)\n",
    "                        sampled_action_corr = dist.sample()\n",
    "                        sampled_action_decoded_corr, log_prob_corr = decode_action(dist, sampled_action_corr)\n",
    "\n",
    "                        # compute the probabilty of the action sampled with the current policy with respect to the old policy (for the ro in bias correction)\n",
    "                        seg_dist = compute_dist(b_seg_action_params)\n",
    "                        _, seg_log_prob_corr = decode_action(seg_dist, sampled_action_corr)\n",
    "\n",
    "                        # compute the value functions\n",
    "                        _, Vs, hs = target_V(b_state)\n",
    "                        _, Qs, _ = target_Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "                        _, Qs_corr, _ = target_Q(torch.concat([hs, sampled_action_decoded_corr],-1))\n",
    "\n",
    "                        b_V_t = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "                        b_Q_t = (torch.softmax(Vs+Qs,1)@bin_values[:,None])[:,0]\n",
    "\n",
    "                        b_Q_t_corr = (torch.softmax(Vs+Qs_corr,1)@bin_values[:,None])[:,0]\n",
    "                        b_action_corr = sampled_action_corr\n",
    "\n",
    "                        update_batched( [V_t, Q_t, probs_t, Q_t_corr, action_corr, probs_corr, probs_seg_corr],\n",
    "                                        [b_V_t, b_Q_t, log_prob.exp(), b_Q_t_corr, b_action_corr, log_prob_corr.exp(), seg_log_prob_corr.exp()],\n",
    "                                        batch_size, b_idx)\n",
    "\n",
    "                    # reshape to expose the sequences\n",
    "                    [V_t, Q_t, probs_t, Q_t_corr, action_corr, probs_corr, probs_seg_corr] = reshape_sequences([V_t, Q_t, probs_t, Q_t_corr, action_corr, probs_corr, probs_seg_corr], (replay_batch_size, seg_len))\n",
    "\n",
    "\n",
    "                    # compute targets (as in RETRACE)\n",
    "                    Q_ret = torch.zeros_like(V_t)\n",
    "                    V_target = torch.zeros_like(V_t)\n",
    "\n",
    "                    Q_ret[:,-1] = Q_t[:,-1]\n",
    "\n",
    "                    for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                        ro = (probs_t[:,t+1] + 1e-5)/(seg_action_probs[:,t+1] + 1e-5) # ro of t+1\n",
    "                        ci = lamb * torch.minimum(torch.ones(1, device='cuda'), ro)\n",
    "\n",
    "                        Q_ret[:,t] = seg_rewards[:,t] + gamma*(ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1])\n",
    "                        V_target[:, t+1] = ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1]\n",
    "\n",
    "                    V_target[:, 0] = ci*(Q_ret[:,0] - Q_t[:,0]) + Q_t_corr[:,0]\n",
    "\n",
    "                # unite batch and sequence dimensions\n",
    "                [Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs] = flatten_sequences([Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs], removelast=True)\n",
    "                [Q_t_corr, V_t, action_corr, probs_corr, probs_seg_corr] = flatten_sequences([Q_t_corr, V_t, action_corr, probs_corr, probs_seg_corr], removelast=True)\n",
    "\n",
    "                # run PPO for n epochs\n",
    "                for ppo_epoch in range(experiment['PPO']):\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        # get batch\n",
    "                        [b_Q_ret, b_V_target, b_state, b_action, b_prob, b_prob_seg] = get_batch([Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs], batch_size, b_idx)\n",
    "                        [b_Q_t_corr, b_V_t, b_action_corr, b_probs_corr, b_probs_seg_corr] = get_batch([Q_t_corr, V_t, action_corr, probs_corr, probs_seg_corr], batch_size, b_idx) # bias correction\n",
    "\n",
    "                        # compute action distribution according to current policy\n",
    "                        actions, _, _ = actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        # compute probability of the chosen action with respect to the updated policy\n",
    "                        sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                        # compute probability of the bias-correction action with respect to the updated policy\n",
    "                        _, log_prob_corr = decode_action(dist, b_action_corr)\n",
    "\n",
    "                        # compute the value functions\n",
    "                        _, Vs, hs = V(b_state)\n",
    "                        _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "                        # compute the PPO objective (with respect to the target policy) and apply off policy correction (with bias correction)\n",
    "                        r = (log_prob.exp() + 1e-5 )/(b_prob + 1e-5 )\n",
    "                        adv = (b_prob/b_prob_seg).clip(max = c)*(b_Q_ret - b_V_target)\n",
    "\n",
    "                        r_c = (log_prob_corr.exp() + 1e-5 )/(b_probs_corr + 1e-5)\n",
    "                        adv_c = (1-c*b_probs_seg_corr/b_probs_corr).relu()*(b_Q_t_corr - b_V_t)\n",
    "\n",
    "                        L = torch.minimum(adv*r, adv*r.clip(1-eps,1+eps)) #+ torch.minimum(adv_c*r_c, adv_c*r_c.clip(1-eps,1+eps))\n",
    "\n",
    "                        # approximation of the hentropy\n",
    "                        H = -log_prob.exp()\n",
    "\n",
    "                        actor_error = - L - H*experiment['entropy']\n",
    "\n",
    "                        y_V = two_hot_encode(b_V_target, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        y_Q = two_hot_encode(b_Q_ret, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        critic_error_V = torch.nn.functional.cross_entropy(Vs, y_V, reduction='none')\n",
    "                        critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y_Q, reduction='none')\n",
    "                        critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                        error = actor_error + critic_error\n",
    "\n",
    "                        optim_critic.zero_grad()\n",
    "                        optim_actor.zero_grad() \n",
    "                        error.mean().backward()\n",
    "                        optim_critic.step()\n",
    "                        optim_actor.step()\n",
    "\n",
    "                    update_models(decay=5e-3)\n",
    "\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if (i+1) % validate_every == 0:\n",
    "            # compute the value function and the action probability in each point of the validation plane\n",
    "            Values = []\n",
    "            V_t = []\n",
    "            A = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v, _ = V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                _, vt, _ = target_V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                a, _, _ = actor(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                Values.append(v)\n",
    "                V_t.append(vt)\n",
    "                A.append(a)\n",
    "            Values = torch.concat(Values,0)\n",
    "            A = torch.concat(A,0)\n",
    "            V_t = torch.concat(V_t,0)\n",
    "\n",
    "            # decode values\n",
    "            if use_symlog:\n",
    "                Values = (Values.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "                V_t = (V_t.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "            else:\n",
    "                V = Values.cpu()\n",
    "                V_t = V_t.cpu()\n",
    "\n",
    "            tb_writer.add_image('V', (Values.reshape(1,100,100)/2+0.5), i)\n",
    "            tb_writer.add_image('V_t', V_t.reshape(1,100,100)/2+0.5, i)\n",
    "\n",
    "            # plot the mean.tanh() (not the real mean of the distribution of the tanh)\n",
    "            pos = st[:,0].reshape(100,100,-1)[::2,::2]\n",
    "            A = A.reshape(100,100,-1)[::2,::2]\n",
    "\n",
    "            plt.quiver(pos[...,1].flatten(), -pos[...,0].flatten(), A[...,1].tanh().detach().cpu().flatten(), -A[...,0].tanh().detach().cpu().flatten(), color='g',scale=50, headwidth=2)\n",
    "            ax.axis('off')\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.subplots_adjust(0,0,1,1,0,0)\n",
    "            fig.canvas.draw()\n",
    "            data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.clf()\n",
    "\n",
    "            tb_writer.add_image('Policy visualization', np.transpose(data,(2,0,1)) , i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
