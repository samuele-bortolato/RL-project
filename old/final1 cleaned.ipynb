{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir('../input/rl-project')\n",
    "# import sys\n",
    "# sys.path.insert(0,'../input/rl-project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from agents import Agent\n",
    "from environment import SimulationEnvironment0\n",
    "from replay_buffers import *\n",
    "from utils import *\n",
    "\n",
    "import copy\n",
    "experiment_name='final_1_batch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x2d2bd720820>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_discrete(actions):\n",
    "    action_probs = actions.softmax(-1)\n",
    "    dist = Categorical(action_probs)\n",
    "    return dist\n",
    "\n",
    "def compute_dist_continuous(actions):\n",
    "    \n",
    "    means = actions[:,:2]\n",
    "    correlations = actions[:,2:-2].tanh()*(1-1e-5)\n",
    "    variances = actions[:,-2:].exp()\n",
    "\n",
    "    b,n = means.shape\n",
    "    corr_matrix = torch.zeros(b, n, n, device=means.device)\n",
    "    indices = torch.tril_indices(n, n, offset=-1)\n",
    "    corr_matrix[..., indices[0], indices[1]] = correlations\n",
    "    corr_matrix[..., indices[1], indices[0]] = correlations\n",
    "    corr_matrix[..., torch.arange(n), torch.arange(n)] = 1\n",
    "    cov_matrix = corr_matrix * variances[...,None] * variances[:,None]\n",
    "    \n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(means, cov_matrix)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action_discrete(dist, sampled_action):\n",
    "    # WARNING: uses global dec_x dec_y to decode the actions!\n",
    "    log_prob = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = torch.stack([dec_x[sampled_action],dec_y[sampled_action]],1)\n",
    "    return sampled_action_decoded, log_prob\n",
    "\n",
    "def decode_action_continuous(dist, sampled_action):\n",
    "    log_prob_sample = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = sampled_action.tanh()\n",
    "    log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1) # correct accounting for the tanh transform\n",
    "    return sampled_action_decoded, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_models(decay=1e-3):\n",
    "    update_target_model(model=actor, target_model=target_actor,decay=decay)\n",
    "    update_target_model(model=V, target_model=target_V, decay=decay)\n",
    "    update_target_model(model=Q, target_model=target_Q, decay=decay)\n",
    "\n",
    "def flatten_sequences(X, removelast=False):\n",
    "    for i in range(len(X)):\n",
    "        if removelast:\n",
    "            X[i] = X[i][:,:-1]\n",
    "        s = X[i].shape\n",
    "        if len(s)==2:\n",
    "            X[i] = X[i].reshape(s[0]*s[1])\n",
    "        else:\n",
    "            X[i] = X[i].reshape(s[0]*s[1],-1)\n",
    "    return X\n",
    "\n",
    "def reshape_sequences(X, shape):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i].reshape(shape)\n",
    "    return X\n",
    "\n",
    "def get_batch(X, batch_size, b_idx):\n",
    "    start = b_idx*batch_size\n",
    "    end = (b_idx+1)*batch_size\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i][start:end]\n",
    "    return X\n",
    "\n",
    "def initialize_zeros(shape, n, device):\n",
    "    X=[]\n",
    "    for _ in range(n):\n",
    "        X.append(torch.zeros(shape, device=device))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 131072/131072 [33:11<00:00, 65.81it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "EXPERIMENTS = [#{\"entropy\": 3e-4, 'PPO':4, 'replay_ratio':0, 'training_steps':2**17},\n",
    "               {\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':1, 'training_steps':2**17},\n",
    "               #{\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':3, 'training_steps':2**17}\n",
    "               ]\n",
    "\n",
    "\n",
    "# simulation\n",
    "num_simulations = 128\n",
    "num_blackholes = 1\n",
    "\n",
    "# agent\n",
    "hidden_size = 512\n",
    "simlog_res = 255\n",
    "use_symlog = True\n",
    "discrete_actions = False\n",
    "simlog_half_res = simlog_res//2\n",
    "simlog_max_range = 1\n",
    "actions_res = 5\n",
    "levels=2\n",
    "input_type = 'complete'\n",
    "\n",
    "lr = 3e-4\n",
    "lr_actor = 3e-5\n",
    "\n",
    "\n",
    "\n",
    "# training\n",
    "#training_steps = 2**18\n",
    "#epochs=8\n",
    "lamb = 0.8\n",
    "gamma = 0.98\n",
    "smoothing = 1e-2\n",
    "eps = 0.05 # for PPO update\n",
    "seg_len = 2**5\n",
    "h_samples = 0\n",
    "c=10\n",
    "\n",
    "\n",
    "# replay buffers\n",
    "num_steps = 1024\n",
    "batch_size = 2**10\n",
    "replay_batch_size = num_simulations\n",
    "\n",
    "plot = False\n",
    "\n",
    "validate_every = 2**9\n",
    "\n",
    "bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n",
    "bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n",
    "\n",
    "#dec_x, dec_y = torch.meshgrid(torch.arange(actions_res)/(actions_res-1)*2-1, torch.arange(actions_res)/(actions_res-1)*2-1)\n",
    "#dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n",
    "\n",
    "metric_idx = torch.pow(2,torch.arange(15))-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "for experiment in EXPERIMENTS:\n",
    "\n",
    "    # SETUP EXPERIMENT\n",
    "\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # initialize logging (with tensorboard)\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",experiment_name, current_time + \"_\" + socket.gethostname() \n",
    "    )\n",
    "    tb_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # initialize the simulation\n",
    "    sim = SimulationEnvironment0(num_simulations=num_simulations,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            max_steps=250,\n",
    "                            device='cuda')\n",
    "    states = sim.get_state()\n",
    "\n",
    "    # initialize the networks\n",
    "    if discrete_actions:\n",
    "        action_dim = actions_res**2\n",
    "        compute_dist = compute_dist_discrete\n",
    "        decode_action = decode_action_discrete\n",
    "    else:\n",
    "        action_dim = 5\n",
    "        compute_dist = compute_dist_continuous\n",
    "        decode_action = decode_action_continuous\n",
    "\n",
    "    if use_symlog:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=simlog_res).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=simlog_res).cuda()\n",
    "    else:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=1).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=1).cuda()\n",
    "\n",
    "    optim_actor = torch.optim.AdamW(list(actor.parameters())+list(actor2.parameters()), lr=lr_actor, weight_decay=1e-3)\n",
    "    optim_critic = torch.optim.AdamW(list(V.parameters())+list(Q.parameters()), lr=lr, weight_decay=1e-3)\n",
    "    target_actor = copy.deepcopy(actor)\n",
    "    target_V = copy.deepcopy(V)\n",
    "    target_Q = copy.deepcopy(Q)\n",
    "\n",
    "    # initialize the replay buffer\n",
    "    replay_buffer = Replay_Buffer_Segments(state_shape=((num_blackholes+2),2), action_shape=(2,), params_shape=(action_dim,), segment_lenght=seg_len, num_simulations=num_simulations, num_steps=num_steps, batch_size=replay_batch_size, device='cuda')\n",
    "\n",
    "    # initialize vailidation plane\n",
    "    x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n",
    "    pos = torch.stack([x.flatten(), y.flatten()],1)/100\n",
    "\n",
    "    target_pos = torch.ones_like(pos)*0.25      # position of the target\n",
    "    bh_pos = torch.ones_like(pos)*0.75          # position of the blackholes\n",
    "\n",
    "    st=torch.stack([pos,target_pos,bh_pos],1)\n",
    "\n",
    "\n",
    "    # START EXPERIMENT\n",
    "    pbar = tqdm(range(experiment['training_steps']))\n",
    "    for i in pbar:\n",
    "        t0=time.time()\n",
    "\n",
    "        # GENERATE EXPERIENCE\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            states = states.reshape(states.shape[0],-1).cuda()\n",
    "\n",
    "            # compute action distribution according to the current policy\n",
    "            actions, _, _ = actor(states)\n",
    "            dist = compute_dist(actions)\n",
    "\n",
    "            # sample an action\n",
    "            sampled_action = dist.sample()\n",
    "            sampled_action_decoded, log_prob = decode_action(dist, sampled_action)\n",
    "\n",
    "            # simulation step\n",
    "            rewards, new_states, terminals = sim.step(sampled_action_decoded)\n",
    "\n",
    "            # save experience\n",
    "            replay_buffer.add_experience(states.reshape(new_states.shape), sampled_action, actions, log_prob.exp(), rewards, terminals)\n",
    "\n",
    "        tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n",
    "\n",
    "        if (i+1) % seg_len == 0:\n",
    "\n",
    "            # ON POLICY UPDATE\n",
    "\n",
    "            seg_state, seg_actions, _, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_last_segment()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "\n",
    "                # get target values for all the states in the segment\n",
    "                V_t = torch.zeros(num_simulations*seg_len, device='cuda')\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    b_state = seg_state.reshape((seg_len)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    \n",
    "                    _, Vs, _= target_V(b_state)\n",
    "                    V_t[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "        \n",
    "                V_t = V_t.reshape(num_simulations, seg_len)\n",
    "\n",
    "                # compute GAE and TD lambda returns\n",
    "                gae = torch.zeros_like(V_t)\n",
    "                for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                    d_t = -V_t[:,t] + seg_rewards[:,t] + gamma*V_t[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                    gae[:,t] = d_t + gamma*lamb*gae[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                tdl = V_t + gae\n",
    "\n",
    "            # unite batch and sequence dimensions\n",
    "            [gae, tdl, seg_state, seg_actions, seg_action_probs] = flatten_sequences([gae, tdl, seg_state, seg_actions, seg_action_probs], removelast=True)\n",
    "\n",
    "            # run PPO for n epochs\n",
    "            for ppo_epoch in range(experiment['PPO']):\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    # get batch\n",
    "                    [b_gae, b_tdl, b_state, b_action, b_prob] = get_batch([gae, tdl, seg_state, seg_actions, seg_action_probs], batch_size, b_idx)\n",
    "\n",
    "                    # compute action distribution according to the updated policy\n",
    "                    actions, _, _ = actor(b_state)\n",
    "                    dist = compute_dist(actions)\n",
    "\n",
    "                    # compute probability of the chosen action with respect to the updated policy\n",
    "                    sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                    # compute the value functions\n",
    "                    _, Vs, hs = V(b_state)\n",
    "                    _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "                    # compute PPO clip objective\n",
    "                    r = (log_prob.exp() + 1e-5)/(b_prob + 1e-5)\n",
    "                    L = torch.minimum(b_gae*r, b_gae*r.clip(1-eps,1+eps))\n",
    "\n",
    "                    # approximation of the hentropy\n",
    "                    H = -log_prob.exp()\n",
    "\n",
    "                    actor_error = - L - H*experiment['entropy']\n",
    "\n",
    "                    y = two_hot_encode(b_tdl, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                    critic_error_V = torch.nn.functional.cross_entropy(Vs, y, reduction='none')\n",
    "                    critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y, reduction='none')\n",
    "                    critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                    error = actor_error + critic_error\n",
    "\n",
    "                    optim_critic.zero_grad()\n",
    "                    optim_actor.zero_grad() \n",
    "                    error.mean().backward()\n",
    "                    optim_critic.step()\n",
    "                    optim_actor.step()\n",
    "                \n",
    "                update_models(decay=5e-3)\n",
    "\n",
    "            tb_writer.add_scalar('critic_error_Q',critic_error_Q.mean().item()-critic_error_V.mean().item(), i)\n",
    "            tb_writer.add_scalar('hentropy',H.mean().item(), i)\n",
    "\n",
    "            # REPLAY EXPERIENCES\n",
    "\n",
    "            # replay experiences\n",
    "            for replay_epoch in range(experiment['replay_ratio']):\n",
    "\n",
    "                seg_state, seg_actions, seg_action_params, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_batch()\n",
    "\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    # get target values and action probs for all the states in the segment\n",
    "\n",
    "                    V_t, Q_t, probs_t, Q_t_corr, ro_corr = initialize_zeros(replay_batch_size*seg_len, n=5, device='cuda')\n",
    "\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        # get batch\n",
    "                        [b_state, b_action, b_seg_action_params] = get_batch(flatten_sequences([seg_state, seg_actions, seg_action_params]), batch_size, b_idx)\n",
    "\n",
    "                        # compute action distribution according to current policy\n",
    "                        actions, _, _ = target_actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        # compute the probability of the replayed action\n",
    "                        sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                        # sample a new action from the current policy (for the bias correction)\n",
    "                        sampled_action_corr = dist.sample()\n",
    "                        sampled_action_decoded_corr, log_prob_corr = decode_action(dist, sampled_action_corr)\n",
    "\n",
    "                        # compute the probabilty of the action sampled with the current policy with respect to the old policy (for the ro in bias correction)\n",
    "                        seg_dist = compute_dist(b_seg_action_params)\n",
    "                        _, seg_log_prob_corr = decode_action(seg_dist, sampled_action_corr)\n",
    "\n",
    "                        # compute the value functions\n",
    "                        _, Vs, hs = target_V(b_state)\n",
    "                        _, Qs, _ = target_Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "                        _, Qs_corr, _ = target_Q(torch.concat([hs, sampled_action_decoded_corr],-1))\n",
    "\n",
    "                        start = b_idx*batch_size\n",
    "                        end = (b_idx+1)*batch_size\n",
    "\n",
    "                        V_t[start:end] = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "                        Q_t[start:end] = (torch.softmax(Vs+Qs,1)@bin_values[:,None])[:,0]\n",
    "                        probs_t[start:end] = log_prob.exp()\n",
    "                        Q_t_corr[start:end] = (torch.softmax(Vs+Qs_corr,1)@bin_values[:,None])[:,0]\n",
    "                        ro_corr[start:end] = (log_prob_corr.exp() + 1e-5)/(seg_log_prob_corr.exp() + 1e-5)\n",
    "\n",
    "                    # reshape to expose the sequences\n",
    "                    [V_t, Q_t, probs_t, Q_t_corr, ro_corr] = reshape_sequences([V_t, Q_t, probs_t, Q_t_corr, ro_corr], (replay_batch_size, seg_len))\n",
    "\n",
    "                    # compute targets (as in RETRACE)\n",
    "\n",
    "                    Q_ret = torch.zeros_like(V_t)\n",
    "                    V_target = torch.zeros_like(V_t)\n",
    "                    corr = (1-c/ro_corr).relu()*(Q_t_corr - V_t)\n",
    "\n",
    "                    Q_ret[:,-1] = Q_t[:,-1]\n",
    "\n",
    "                    for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                        ro = (probs_t[:,t+1] + 1e-5)/(seg_action_probs[:,t+1] + 1e-5) # ro of t+1\n",
    "                        ci = lamb * torch.minimum(torch.ones(1, device='cuda'), ro)\n",
    "\n",
    "                        Q_ret[:,t] = seg_rewards[:,t] + gamma*(ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1])\n",
    "                        V_target[:, t+1] = ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1]\n",
    "\n",
    "                    V_target[:, 0] = ci*(Q_ret[:,0] - Q_t[:,0]) + V_t[:,0]\n",
    "\n",
    "                # unite batch and sequence dimensions\n",
    "                [Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs, corr] = flatten_sequences([Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs, corr], removelast=True)\n",
    "\n",
    "                # run PPO for n epochs\n",
    "                for ppo_epoch in range(experiment['PPO']):\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        # get batch\n",
    "                        [b_Q_ret, b_V_target, b_state, b_action, b_prob, b_prob_seg, b_corr] = get_batch([Q_ret, V_target, seg_state, seg_actions, probs_t, seg_action_probs, corr], batch_size, b_idx)\n",
    "\n",
    "                        # compute action distribution according to current policy\n",
    "                        actions, _, _ = actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        # compute probability of the chosen action with respect to the updated policy\n",
    "                        sampled_action_decoded, log_prob = decode_action(dist, b_action)\n",
    "\n",
    "                        # compute the value functions\n",
    "                        _, Vs, hs = V(b_state)\n",
    "                        _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "                        # compute the PPO objective (with respect to the target policy) and apply off policy correction (with bias correction)\n",
    "                        r = (log_prob.exp() + 1e-5 )/(b_prob + 1e-5 )\n",
    "                        adv = (b_prob/b_prob_seg).clip(max = c)*(b_Q_ret - b_V_target)# + b_corr\n",
    "                        L = torch.minimum(adv*r, adv*r.clip(1-eps,1+eps))\n",
    "\n",
    "                        # approximation of the hentropy\n",
    "                        H = -log_prob.exp()\n",
    "\n",
    "                        actor_error = - L - H*experiment['entropy']\n",
    "\n",
    "                        y_V = two_hot_encode(b_V_target, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        y_Q = two_hot_encode(b_Q_ret, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        critic_error_V = torch.nn.functional.cross_entropy(Vs, y_V, reduction='none')\n",
    "                        critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y_Q, reduction='none')\n",
    "                        critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                        error = actor_error + critic_error\n",
    "\n",
    "                        optim_critic.zero_grad()\n",
    "                        optim_actor.zero_grad() \n",
    "                        error.mean().backward()\n",
    "                        optim_critic.step()\n",
    "                        optim_actor.step()\n",
    "\n",
    "                    update_models(decay=5e-3)\n",
    "\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if (i+1) % validate_every == 0:\n",
    "\n",
    "            # compute the value function and the action probability in each point of the validation plane\n",
    "            Values = []\n",
    "            V_t = []\n",
    "            A = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v, _ = V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                _, vt, _ = target_V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                a, _, _ = actor(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                Values.append(v)\n",
    "                V_t.append(vt)\n",
    "                A.append(a)\n",
    "            Values = torch.concat(Values,0)\n",
    "            A = torch.concat(A,0)\n",
    "            V_t = torch.concat(V_t,0)\n",
    "\n",
    "            # decode values\n",
    "            if use_symlog:\n",
    "                Values = (Values.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "                V_t = (V_t.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "            else:\n",
    "                V = Values.cpu()\n",
    "                V_t = V_t.cpu()\n",
    "\n",
    "            tb_writer.add_image('V', (Values.reshape(1,100,100)/2+0.5), i)\n",
    "            tb_writer.add_image('V_t', V_t.reshape(1,100,100)/2+0.5, i)\n",
    "\n",
    "            # plot the mean.tanh() (not the real mean of the distribution of the tanh)\n",
    "            pos = st[:,0].reshape(100,100,-1)[::2,::2]\n",
    "            A = A.reshape(100,100,-1)[::2,::2]\n",
    "\n",
    "            plt.quiver(pos[...,1].flatten(), -pos[...,0].flatten(), A[...,1].tanh().detach().cpu().flatten(), -A[...,0].tanh().detach().cpu().flatten(), color='g',scale=50, headwidth=2)\n",
    "            ax.axis('off')\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.subplots_adjust(0,0,1,1,0,0)\n",
    "            fig.canvas.draw()\n",
    "            data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.clf()\n",
    "\n",
    "            tb_writer.add_image('Policy visualization', np.transpose(data,(2,0,1)) , i)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
