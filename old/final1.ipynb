{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir('../input/rl-project')\n",
    "# import sys\n",
    "# sys.path.insert(0,'../input/rl-project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from agents import Agent\n",
    "from environment import SimulationEnvironment0\n",
    "from replay_buffers import *\n",
    "from utils import *\n",
    "\n",
    "import copy\n",
    "experiment_name='final_1_dist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x27786a7b250>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist_discrete(actions):\n",
    "    action_probs = actions.softmax(-1)\n",
    "    dist = Categorical(action_probs)\n",
    "    return dist\n",
    "\n",
    "def compute_dist_continuous(actions):\n",
    "    \n",
    "    means = actions[:,:2]\n",
    "    correlations = actions[:,2:-2].tanh()*(1-1e-5)\n",
    "    variances = actions[:,-2:].exp()\n",
    "\n",
    "    b,n = means.shape\n",
    "    corr_matrix = torch.zeros(b, n, n, device=means.device)\n",
    "    indices = torch.tril_indices(n, n, offset=-1)\n",
    "    corr_matrix[..., indices[0], indices[1]] = correlations\n",
    "    corr_matrix[..., indices[1], indices[0]] = correlations\n",
    "    corr_matrix[..., torch.arange(n), torch.arange(n)] = 1\n",
    "    cov_matrix = corr_matrix * variances[...,None] * variances[:,None]\n",
    "    \n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(means, cov_matrix)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action_discrete(dist, sampled_action):\n",
    "    # WARNING: uses global dec_x dec_y to decode the actions!\n",
    "    log_prob = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = torch.stack([dec_x[sampled_action],dec_y[sampled_action]],1)\n",
    "    return sampled_action_decoded, log_prob\n",
    "\n",
    "def decode_action_continuous(dist, sampled_action):\n",
    "    log_prob_sample = dist.log_prob(sampled_action)\n",
    "    sampled_action_decoded = sampled_action.tanh()\n",
    "    log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1) # correct accounting for the tanh transform\n",
    "    return sampled_action_decoded, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_models(decay=1e-3):\n",
    "    update_target_model(model=actor, target_model=target_actor,decay=decay)\n",
    "    update_target_model(model=V, target_model=target_V, decay=decay)\n",
    "    update_target_model(model=Q, target_model=target_Q, decay=decay)\n",
    "\n",
    "def flatten_sequences(X, removelast=False):\n",
    "    for i in range(len(X)):\n",
    "        if removelast:\n",
    "            X[i] = X[i][:,:-1]\n",
    "        s = X[i].shape\n",
    "        if len(s)==2:\n",
    "            X[i] = X[i].reshape(s[0]*s[1])\n",
    "        else:\n",
    "            X[i] = X[i].reshape(s[0]*s[1],-1)\n",
    "    return X\n",
    "\n",
    "def reshape_sequences(X, shape):\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i].reshape(shape)\n",
    "    return X\n",
    "\n",
    "def get_batch(X, batch_size, b_idx):\n",
    "    start = b_idx*batch_size\n",
    "    end = (b_idx+1)*batch_size\n",
    "    for i in range(len(X)):\n",
    "        X[i] = X[i][start:end]\n",
    "    return X\n",
    "\n",
    "def initialize_zeros(shape, n, device):\n",
    "    X=[]\n",
    "    for _ in range(n):\n",
    "        X.append(torch.zeros(shape, device=device))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131072/131072 [55:11<00:00, 39.58it/s] \n",
      "100%|██████████| 131072/131072 [1:58:31<00:00, 18.43it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 0\n",
    "\n",
    "EXPERIMENTS = [#{\"entropy\": 3e-4, 'PPO':4, 'replay_ratio':0, 'training_steps':2**17},\n",
    "               {\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':1, 'training_steps':2**17},\n",
    "               {\"entropy\": 3e-4, 'PPO':2, 'replay_ratio':3, 'training_steps':2**17}\n",
    "               ]\n",
    "\n",
    "\n",
    "# simulation\n",
    "num_simulations = 128\n",
    "num_blackholes = 1\n",
    "\n",
    "# agent\n",
    "hidden_size = 512\n",
    "simlog_res = 255\n",
    "use_symlog = True\n",
    "discrete_actions = False\n",
    "simlog_half_res = simlog_res//2\n",
    "simlog_max_range = 1\n",
    "actions_res = 5\n",
    "levels=2\n",
    "input_type = 'complete'\n",
    "\n",
    "lr = 3e-4\n",
    "lr_actor = 3e-5\n",
    "\n",
    "\n",
    "\n",
    "# training\n",
    "#training_steps = 2**18\n",
    "#epochs=8\n",
    "lamb = 0.8\n",
    "gamma = 0.98\n",
    "smoothing = 1e-2\n",
    "eps = 0.05 # for PPO update\n",
    "seg_len = 2**5\n",
    "h_samples = 0\n",
    "c=10\n",
    "\n",
    "\n",
    "# replay buffers\n",
    "num_steps = 1024\n",
    "batch_size = 2**10\n",
    "replay_batch_size = num_simulations\n",
    "\n",
    "plot = False\n",
    "\n",
    "validate_every = 2**9\n",
    "\n",
    "bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n",
    "bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n",
    "\n",
    "#dec_x, dec_y = torch.meshgrid(torch.arange(actions_res)/(actions_res-1)*2-1, torch.arange(actions_res)/(actions_res-1)*2-1)\n",
    "#dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n",
    "\n",
    "metric_idx = torch.pow(2,torch.arange(15))-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "\n",
    "\n",
    "for experiment in EXPERIMENTS:\n",
    "\n",
    "    # set seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # initialize logging (with tensorboard)\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",experiment_name, current_time + \"_\" + socket.gethostname() \n",
    "    )\n",
    "    tb_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # initialize the simulation\n",
    "    sim = SimulationEnvironment0(num_simulations=num_simulations,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            max_steps=250,\n",
    "                            device='cuda')\n",
    "    states = sim.get_state()\n",
    "\n",
    "    # initialize the networks\n",
    "    if discrete_actions:\n",
    "        action_dim = actions_res**2\n",
    "        compute_dist = compute_dist_discrete\n",
    "        decode_action = decode_action_discrete\n",
    "    else:\n",
    "        action_dim = 5\n",
    "        compute_dist = compute_dist_continuous\n",
    "        decode_action = decode_action_continuous\n",
    "\n",
    "    if use_symlog:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=simlog_res).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=simlog_res).cuda()\n",
    "    else:\n",
    "        actor = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        actor2 = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, critic=False, action_dimension=action_dim).cuda()\n",
    "        V = Agent((num_blackholes+2)*2, hidden_size, levels, input_type, actor=False, value_dimension=1).cuda()\n",
    "        Q = Agent(hidden_size + 2, hidden_size, levels, input_type='base', actor=False, value_dimension=1).cuda()\n",
    "\n",
    "    optim_actor = torch.optim.AdamW(list(actor.parameters())+list(actor2.parameters()), lr=lr_actor, weight_decay=1e-3)\n",
    "    optim_critic = torch.optim.AdamW(list(V.parameters())+list(Q.parameters()), lr=lr, weight_decay=1e-3)\n",
    "    target_actor = copy.deepcopy(actor)\n",
    "    target_V = copy.deepcopy(V)\n",
    "    target_Q = copy.deepcopy(Q)\n",
    "\n",
    "    # initialize the replay buffer\n",
    "    replay_buffer = Replay_Buffer_Segments(state_shape=((num_blackholes+2),2), action_shape=(2,), params_shape=(6,), segment_lenght=seg_len, num_simulations=num_simulations, num_steps=num_steps, batch_size=replay_batch_size, device='cuda')\n",
    "\n",
    "    # initialize vailidation plane\n",
    "    x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n",
    "    pos = torch.stack([x.flatten(), y.flatten()],1)/100\n",
    "\n",
    "    target_pos = torch.ones_like(pos)*0.25      # position of the target\n",
    "    bh_pos = torch.ones_like(pos)*0.75          # position of the blackholes\n",
    "\n",
    "    st=torch.stack([pos,target_pos,bh_pos],1)\n",
    "\n",
    "    pbar = tqdm(range(experiment['training_steps']))\n",
    "    for i in pbar:\n",
    "        t0=time.time()\n",
    "\n",
    "        # GENERATE EXPERIENCE\n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            states = states.reshape(states.shape[0],-1).cuda()\n",
    "\n",
    "            actions, _, _ = actor(states)\n",
    "            dist = compute_dist(actions)\n",
    "\n",
    "            sampled_action = dist.sample()\n",
    "            log_prob_sample = dist.log_prob(sampled_action)\n",
    "\n",
    "            # apply the tanh trans\n",
    "            sampled_action_decoded = sampled_action.tanh()\n",
    "            log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1)\n",
    "\n",
    "            rewards, new_states, terminals = sim.step(sampled_action_decoded)\n",
    "\n",
    "            replay_buffer.add_experience(states.reshape(new_states.shape), sampled_action, torch.concat([cov_matrix.flatten(1),means],-1), log_prob.exp(), rewards, terminals)\n",
    "\n",
    "        tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n",
    "\n",
    "        if (i+1) % seg_len == 0:\n",
    "\n",
    "            # ON POLICY UPDATE\n",
    "\n",
    "            seg_state, seg_actions, _, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_last_segment()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "\n",
    "                # get target values for all the states in the segment\n",
    "                V_t = torch.zeros(num_simulations*seg_len, device='cuda')\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    b_state = seg_state.reshape((seg_len)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    \n",
    "                    _, Vs, _= target_V(b_state)\n",
    "                    V_t[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "        \n",
    "                V_t = V_t.reshape(num_simulations, seg_len)\n",
    "\n",
    "                # compute GAE and TD lambda returns\n",
    "                gae = torch.zeros_like(V_t)\n",
    "                for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                    d_t = -V_t[:,t] + seg_rewards[:,t] + gamma*V_t[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                    gae[:,t] = d_t + gamma*lamb*gae[:,t+1]*seg_terminal[:,t].logical_not()\n",
    "\n",
    "                tdl = V_t + gae\n",
    "\n",
    "\n",
    "            #with torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "            # run PPO for n epochs\n",
    "            for ppo_epoch in range(experiment['PPO']):\n",
    "                for b_idx in range((seg_len*num_simulations+batch_size-1)//batch_size):\n",
    "\n",
    "                    b_gae = gae[:,:-1].reshape((seg_len-1)*num_simulations)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    b_tdl = tdl[:,:-1].reshape((seg_len-1)*num_simulations)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    b_state = seg_state[:,:-1].reshape((seg_len-1)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    b_action = seg_actions[:,:-1].reshape((seg_len-1)*num_simulations,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                    b_prob = seg_action_probs[:,:-1].reshape((seg_len-1)*num_simulations)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "\n",
    "                    actions, _, _ = actor(b_state)\n",
    "                    dist = compute_dist(actions)\n",
    "\n",
    "                    sampled_action_decoded = b_action.tanh()\n",
    "                    _, Vs, hs = V(b_state)\n",
    "                    _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "\n",
    "                    assert not torch.any(torch.isnan(actions))\n",
    "                    assert not torch.any(torch.isnan(Vs))\n",
    "                    assert not torch.any(torch.isnan(Qs))\n",
    "                    assert not torch.any(torch.isinf(actions))\n",
    "                    assert not torch.any(torch.isinf(Vs))\n",
    "                    assert not torch.any(torch.isinf(Qs))\n",
    "\n",
    "                    \n",
    "                    log_prob_sample = dist.log_prob(b_action)\n",
    "                    log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1)\n",
    "                    probs = log_prob.exp()\n",
    "\n",
    "                    r = (probs + 1e-5)/(b_prob + 1e-5)\n",
    "                    L = torch.minimum(b_gae*r, b_gae*r.clip(1-eps,1+eps))\n",
    "\n",
    "                    # # approximation of the hentropy\n",
    "                    if h_samples>0:\n",
    "                        s = dist.sample((h_samples,))\n",
    "                        log_prob = dist.log_prob(s) - torch.log(1 - s.tanh().square() + 1e-8).sum(-1)\n",
    "                        H = 0.5 -log_prob.exp().mean(0)\n",
    "                    else:\n",
    "                        H = -log_prob.exp()\n",
    "\n",
    "                    actor_error = - L - H*experiment['entropy'] #- L2 + (variances2+1e-8).log().square().mean(-1)\n",
    "\n",
    "                    y = two_hot_encode(b_tdl, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                    critic_error_V = torch.nn.functional.cross_entropy(Vs, y, reduction='none')\n",
    "                    critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y, reduction='none')\n",
    "                    critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                    error = actor_error + critic_error\n",
    "\n",
    "                    assert not torch.any(torch.isnan(actor_error))\n",
    "                    assert not torch.any(torch.isnan(critic_error))\n",
    "                    assert not torch.any(torch.isinf(actor_error))\n",
    "                    assert not torch.any(torch.isinf(critic_error))\n",
    "\n",
    "                    optim_critic.zero_grad()\n",
    "                    critic_error.mean().backward(retain_graph=True)\n",
    "                    optim_actor.zero_grad() # also delete the derivative of the critic with respect to the actor through Q\n",
    "                    # for param in Q.parameters(): # turn off the gradient saving for the critic to not update Q with the actor loss\n",
    "                    #     param.requires_grad = False\n",
    "                    actor_error.mean().backward()\n",
    "                    # for param in Q.parameters():\n",
    "                    #     param.requires_grad = True\n",
    "                    optim_critic.step()\n",
    "                    optim_actor.step()\n",
    "                \n",
    "                update_target_model(model=actor, target_model=target_actor, decay=5e-3)\n",
    "                update_target_model(model=V, target_model=target_V, decay=5e-3)\n",
    "                update_target_model(model=Q, target_model=target_Q, decay=5e-3)\n",
    "            tb_writer.add_scalar('critic_error_Q',critic_error_Q.mean().item()-critic_error_V.mean().item(), i)\n",
    "            tb_writer.add_scalar('hentropy',H.mean().item(), i)\n",
    "\n",
    "            # REPLAY EXPERIENCES\n",
    "\n",
    "            # replay experiences\n",
    "            for replay_epoch in range(experiment['replay_ratio']):\n",
    "\n",
    "                seg_state, seg_actions, seg_action_params, seg_action_probs, seg_rewards, seg_terminal = replay_buffer.get_batch()\n",
    "\n",
    "                with torch.inference_mode():\n",
    "\n",
    "                    # get target values and action probs for all the states in the segment\n",
    "\n",
    "                    V_t = torch.zeros(replay_batch_size*seg_len, device='cuda')\n",
    "                    Q_t = torch.zeros(replay_batch_size*seg_len, device='cuda')\n",
    "                    probs_t = torch.zeros(replay_batch_size*seg_len, device='cuda')\n",
    "                    Q_t_corr = torch.zeros(replay_batch_size*seg_len, device='cuda')\n",
    "                    ro_corr = torch.zeros(replay_batch_size*seg_len, device='cuda')\n",
    "\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        b_state = seg_state.reshape((seg_len)*replay_batch_size,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_action = seg_actions.reshape((seg_len)*replay_batch_size,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_seg_action_params = seg_action_params.reshape((seg_len)*replay_batch_size,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "\n",
    "                        actions, _, _ = target_actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        log_prob_sample = dist.log_prob(b_action)\n",
    "                        sampled_action_decoded = b_action.tanh()\n",
    "                        log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1)\n",
    "\n",
    "                        sampled_action_corr = dist.sample()\n",
    "                        log_prob_sample_corr = dist.log_prob(sampled_action_corr)\n",
    "                        sampled_action_decoded_corr = sampled_action_corr.tanh()\n",
    "                        log_prob_corr = log_prob_sample_corr - torch.log( 1- sampled_action_decoded_corr.square() + 1e-8 ).sum(-1)\n",
    "\n",
    "                        seg_corr = b_seg_action_params[:,:-2].reshape(-1,2,2)\n",
    "                        seg_means = b_seg_action_params[:,-2:]\n",
    "                        seg_dist = torch.distributions.multivariate_normal.MultivariateNormal(seg_means, seg_corr)\n",
    "                        seg_log_prob_sample_corr = seg_dist.log_prob(sampled_action_corr)\n",
    "                        seg_log_prob_corr = seg_log_prob_sample_corr - torch.log( 1- sampled_action_decoded_corr.square() + 1e-8 ).sum(-1)\n",
    "\n",
    "                        _, Vs, hs = target_V(b_state)\n",
    "                        _, Qs, _ = target_Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "                        _, Qs_corr, _ = target_Q(torch.concat([hs, sampled_action_decoded_corr],-1))\n",
    "\n",
    "                        assert not torch.any(torch.isnan(actions))\n",
    "                        assert not torch.any(torch.isnan(Vs))\n",
    "                        assert not torch.any(torch.isnan(Qs))\n",
    "                        assert not torch.any(torch.isinf(actions))\n",
    "                        assert not torch.any(torch.isinf(Vs))\n",
    "                        assert not torch.any(torch.isinf(Qs))\n",
    "\n",
    "                        V_t[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs,1)@bin_values[:,None])[:,0]\n",
    "                        Q_t[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs+Qs,1)@bin_values[:,None])[:,0]\n",
    "                        probs_t[b_idx*batch_size:(b_idx+1)*batch_size] = log_prob.exp()\n",
    "                        Q_t_corr[b_idx*batch_size:(b_idx+1)*batch_size] = (torch.softmax(Vs+Qs_corr,1)@bin_values[:,None])[:,0]\n",
    "                        ro_corr[b_idx*batch_size:(b_idx+1)*batch_size] = (log_prob_corr.exp() + 1e-5)/(seg_log_prob_corr.exp() + 1e-5)\n",
    "\n",
    "                    V_t = V_t.reshape(replay_batch_size, seg_len)\n",
    "                    Q_t = Q_t.reshape(replay_batch_size, seg_len)\n",
    "                    probs_t = probs_t.reshape(replay_batch_size, seg_len)\n",
    "                    Q_t_corr = Q_t_corr.reshape(replay_batch_size, seg_len)\n",
    "                    ro_corr = ro_corr.reshape(replay_batch_size, seg_len)\n",
    "\n",
    "                    # compute targets (as in RETRACE)\n",
    "\n",
    "                    Q_ret = torch.zeros_like(V_t)\n",
    "                    V_target = torch.zeros_like(V_t)\n",
    "                    corr = (1-c/ro_corr).relu()*(Q_t_corr - V_t)\n",
    "\n",
    "                    Q_ret[:,-1] = Q_t[:,-1]\n",
    "\n",
    "                    for t in reversed(range(seg_len-1)):\n",
    "\n",
    "                        ro = (probs_t[:,t+1] + 1e-5)/(seg_action_probs[:,t+1] + 1e-5) # ro of t+1\n",
    "                        ci = lamb * torch.minimum(torch.ones(1, device='cuda'), ro)\n",
    "                        \n",
    "                        assert not torch.any(torch.isnan(ci))\n",
    "                        assert not torch.any(torch.isnan(ro))\n",
    "                        assert not torch.any(torch.isinf(ci))\n",
    "                        assert not torch.any(torch.isinf(ro))\n",
    "\n",
    "                        Q_ret[:,t] = seg_rewards[:,t] + gamma*(ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1])\n",
    "                        V_target[:, t+1] = ci*(Q_ret[:,t+1] - Q_t[:,t+1]) + V_t[:,t+1]\n",
    "\n",
    "                    V_target[:, 0] = ci*(Q_ret[:,0] - Q_t[:,0]) + V_t[:,0]\n",
    "\n",
    "                \n",
    "                    assert not torch.any(torch.isnan(V_target))\n",
    "                    assert not torch.any(torch.isnan(Q_ret))\n",
    "                    assert not torch.any(torch.isinf(V_target))\n",
    "                    assert not torch.any(torch.isinf(Q_ret))\n",
    "\n",
    "\n",
    "                # run PPO for n epochs\n",
    "                for ppo_epoch in range(experiment['PPO']):\n",
    "                    for b_idx in range((seg_len*replay_batch_size+batch_size-1)//batch_size):\n",
    "\n",
    "                        b_Q_ret = Q_ret[:,:-1].reshape((seg_len-1)*replay_batch_size)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_V_target = V_target[:,:-1].reshape((seg_len-1)*replay_batch_size)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_state = seg_state[:,:-1].reshape((seg_len-1)*replay_batch_size,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_action = seg_actions[:,:-1].reshape((seg_len-1)*replay_batch_size,-1)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_prob = probs_t[:,:-1].reshape((seg_len-1)*replay_batch_size)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_prob_seg = seg_action_probs[:,:-1].reshape((seg_len-1)*replay_batch_size)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "                        b_corr = corr[:,:-1].reshape((seg_len-1)*replay_batch_size)[b_idx*batch_size:(b_idx+1)*batch_size]\n",
    "\n",
    "                        actions, _, _ = actor(b_state)\n",
    "                        dist = compute_dist(actions)\n",
    "\n",
    "                        sampled_action_decoded = b_action.tanh()\n",
    "                        _, Vs, hs = V(b_state)\n",
    "                        _, Qs, _ = Q(torch.concat([hs, sampled_action_decoded],-1))\n",
    "\n",
    "                        assert not torch.any(torch.isnan(actions))\n",
    "                        assert not torch.any(torch.isnan(Vs))\n",
    "                        assert not torch.any(torch.isnan(Qs))\n",
    "                        assert not torch.any(torch.isinf(actions))\n",
    "                        assert not torch.any(torch.isinf(Vs))\n",
    "                        assert not torch.any(torch.isinf(Qs))\n",
    "\n",
    "                        log_prob_sample = dist.log_prob(b_action)\n",
    "                        log_prob = log_prob_sample - torch.log( 1- sampled_action_decoded.square() + 1e-8 ).sum(-1)\n",
    "                        probs = log_prob.exp()\n",
    "\n",
    "                        r = (probs + 1e-5 )/(b_prob + 1e-5 )\n",
    "                        adv = (b_prob/b_prob_seg).clip(0,c)*(b_Q_ret - b_V_target)# + b_corr\n",
    "                        L = torch.minimum(adv*r, adv*r.clip(1-eps,1+eps))\n",
    "\n",
    "                        # approximation of the hentropy\n",
    "                        if h_samples>0:\n",
    "                            s = dist.sample((h_samples,))\n",
    "                            log_prob = dist.log_prob(s) - torch.log(1 - s.tanh().square() + 1e-8).sum(-1)\n",
    "                            H = -log_prob.exp().mean(0)\n",
    "                        else:\n",
    "                            H = -log_prob.exp()\n",
    "\n",
    "                        expected_Q = (torch.softmax(Vs.detach()+Qs,1)@bin_values[:,None])[:,0]\n",
    "                        L2 = torch.minimum(expected_Q*r, expected_Q*r.clip(1-eps,1+eps)) \n",
    "\n",
    "                        actor_error = - L - H*experiment['entropy']# - L2\n",
    "\n",
    "                        y_V = two_hot_encode(b_V_target, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        y_Q = two_hot_encode(b_Q_ret, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "                        critic_error_V = torch.nn.functional.cross_entropy(Vs, y_V, reduction='none')\n",
    "                        critic_error_Q = torch.nn.functional.cross_entropy(Vs+Qs, y_Q, reduction='none')\n",
    "                        critic_error = (critic_error_V + critic_error_Q) / 2\n",
    "                        error = actor_error + critic_error\n",
    "\n",
    "                        assert not torch.any(torch.isnan(actor_error))\n",
    "                        assert not torch.any(torch.isnan(critic_error))\n",
    "                        assert not torch.any(torch.isinf(actor_error))\n",
    "                        assert not torch.any(torch.isinf(critic_error))\n",
    "\n",
    "                        optim_critic.zero_grad()\n",
    "                        critic_error.mean().backward(retain_graph=True)\n",
    "                        optim_critic.step()\n",
    "                        optim_actor.zero_grad() # also delete the derivative of the critic with respect to the actor through Q\n",
    "                        actor_error.mean().backward()\n",
    "                        optim_actor.step()\n",
    "\n",
    "                    update_target_model(model=actor, target_model=target_actor, decay=5e-3)\n",
    "                    update_target_model(model=V, target_model=target_V, decay=5e-3)\n",
    "                    update_target_model(model=Q, target_model=target_Q, decay=5e-3)\n",
    "\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if (i+1) % validate_every == 0:\n",
    "\n",
    "            Values = []\n",
    "            A = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v, _ = V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                a, _, _ = actor(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                Values.append(v)\n",
    "                A.append(a)\n",
    "            Values = torch.concat(Values,0)\n",
    "            A = torch.concat(A,0)\n",
    "\n",
    "            V_t = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "\n",
    "                _, v, _ = target_V(stb.reshape(stb.shape[0],-1).cuda())\n",
    "\n",
    "                V_t.append(v)\n",
    "            V_t = torch.concat(V_t,0)\n",
    "\n",
    "\n",
    "            if use_symlog:\n",
    "                Values = (Values.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "                V_t = (V_t.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "            else:\n",
    "                V = Values.cpu()\n",
    "                V_t = V_t.cpu()\n",
    "\n",
    "            tb_writer.add_image('V', (Values.reshape(1,100,100)/2+0.5), i)\n",
    "            tb_writer.add_image('V_t', V_t.reshape(1,100,100)/2+0.5, i)\n",
    "\n",
    "            pos = st[:,0].reshape(100,100,-1)[::2,::2]\n",
    "            A = A.reshape(100,100,-1)[::2,::2]\n",
    "\n",
    "            plt.quiver(pos[...,1].flatten(), -pos[...,0].flatten(), A[...,1].tanh().detach().cpu().flatten(), -A[...,0].tanh().detach().cpu().flatten(), color='g',scale=50, headwidth=2)\n",
    "            ax.axis('off')\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.subplots_adjust(0,0,1,1,0,0)\n",
    "            fig.canvas.draw()\n",
    "            data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            plt.clf()\n",
    "\n",
    "            tb_writer.add_image('Policy visualization', np.transpose(data,(2,0,1)) , i)\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
