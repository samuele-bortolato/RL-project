{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import socket\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from agents import Agent\n",
    "from replay_buffers import *\n",
    "from utils import *\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import SimulationEnvironment0\n",
    "num_blackholes=1\n",
    "sim = SimulationEnvironment0(num_simulations=128,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            device='cuda')\n",
    "states = sim.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#torch.autograd.set_detect_anomaly(True)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "x,y = torch.meshgrid(torch.arange(1000),torch.arange(1000))\n",
    "pos = torch.stack([x.flatten(), y.flatten()],1)/1000\n",
    "target_pos = torch.ones_like(pos)*0.25\n",
    "bh_pos = torch.ones_like(pos)*0.75\n",
    "\n",
    "st=torch.stack([pos,target_pos,bh_pos],1).cuda()\n",
    "\n",
    "value = ((st[:,0:1]-st[:,2:]).norm(2,-1)<sim.crash_threshold)*sim.crash_reward +  ((st[:,0:1]-st[:,1:2]).norm(2,-1)<sim.goal_threshold)*sim.goal_reward\n",
    "value = value.reshape(1000,1000)\n",
    "\n",
    "ship_positions = st[:, 0, :]\n",
    "goal_position = st[:, 1, :]\n",
    "blackhole_positions = st[:, 2:, :]\n",
    "\n",
    "E=[]\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    actions = torch.rand((1000*1000,2), device='cuda')*2-1\n",
    "\n",
    "    goal_distance_before = torch.norm(ship_positions - goal_position, dim=1)\n",
    "    distance = blackhole_positions - ship_positions.unsqueeze(1)\n",
    "    inv_distance = 1 / torch.norm(distance, dim=2)\n",
    "    direction = distance / torch.norm(distance, dim=2, keepdim=True)\n",
    "    forces = sim.force_constant * direction * inv_distance.unsqueeze(2)\n",
    "    is_crashed = distance.norm(dim=2) < sim.crash_threshold\n",
    "    is_crashed = is_crashed.any(dim=1)\n",
    "    forces[torch.isnan(forces)]=0\n",
    "    forces[is_crashed]=0\n",
    "    actions[is_crashed]=0\n",
    "    ship_velocity = sim.velocity_scale * actions + forces.sum(dim=1)\n",
    "    next_ship_positions = torch.clamp(ship_positions + ship_velocity, 0, 1)\n",
    "\n",
    "    goal_distance_after = torch.norm(next_ship_positions - goal_position, dim=1)\n",
    "    rewards = goal_distance_before - goal_distance_after\n",
    "    is_goal_reached = goal_distance_before < sim.goal_threshold\n",
    "    is_terminal = is_crashed | is_goal_reached\n",
    "\n",
    "    pos, remainder =  (next_ship_positions*1000).floor(), (next_ship_positions*1000).frac()\n",
    "\n",
    "    c0 = value[pos[:,0].clip(0,999).long(),pos[:,1].clip(0,999).long()]\n",
    "    c1 = value[(pos[:,0]+1).clip(0,999).long(),pos[:,1].clip(0,999).long()]\n",
    "    c2 = value[pos[:,0].clip(0,999).long(),(pos[:,1]+1).clip(0,999).long()]\n",
    "    c3 = value[(pos[:,0]+1).clip(0,999).long(),(pos[:,1]+1).clip(0,999).long()]\n",
    "\n",
    "    value_target = (c0*(1-remainder[:,0]) + c1*(remainder[:,0]))*(1-remainder[:,1]) + (c2*(1-remainder[:,0]) + c3*(remainder[:,0]))*(remainder[:,1])\n",
    "    value_target = value_target.reshape(1000,1000)\n",
    "    \n",
    "    error = ((rewards.reshape(1000,1000)+0.98*value_target)-value)\n",
    "    value += 0.2*error*(is_terminal.logical_not()).reshape(1000,1000)\n",
    "    E.append(error.square().mean().item())\n",
    "\n",
    "    # if i%100==99:\n",
    "    #     clear_output()\n",
    "    #     plt.figure(figsize=(10,5))\n",
    "    #     plt.subplot(1,2,1)\n",
    "    #     plt.imshow(value.cpu())\n",
    "    #     plt.subplot(1,2,2)\n",
    "    #     plt.plot(E)\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "value_resized = torch.tensor(cv2.resize(value.cpu().numpy(), (100,100))).reshape(1,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_agents import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**10/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 109570/1048576 [34:29<8:30:25, 30.66it/s, 0.944     0.00781   ]"
     ]
    }
   ],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "LR = [3e-4]#1e-3,5e-4,2.5e-4,1e-4\n",
    "\n",
    "\n",
    "# simulation\n",
    "num_simulations = 2**12\n",
    "num_blackholes = 1\n",
    "\n",
    "# agent\n",
    "hidden_size = 512\n",
    "simlog_res = 255\n",
    "simlog_half_res = simlog_res//2\n",
    "simlog_max_range = 1\n",
    "actions_res = 5\n",
    "levels=2\n",
    "input_type='complete'\n",
    "\n",
    "\n",
    "# training\n",
    "training_steps = 2**20\n",
    "epochs=8\n",
    "gamma = 0.98\n",
    "smoothing = 1e-2\n",
    "eps = 0.05\n",
    "\n",
    "# replay buffers\n",
    "use_prioritized = False\n",
    "size = 2**16\n",
    "batch_size = 2**10\n",
    "\n",
    "\n",
    "plot = False\n",
    "\n",
    "\n",
    "\n",
    "validate_every = 2**7\n",
    "\n",
    "bin_values = (torch.arange(simlog_res)-simlog_half_res).cuda()/simlog_half_res*simlog_max_range\n",
    "bin_values = bin_values.sign()*(bin_values.abs().exp()-1)\n",
    "\n",
    "dec_x, dec_y = torch.meshgrid(torch.arange(5)/2-1, torch.arange(5)/2-1)\n",
    "dec_x, dec_y = dec_x.flatten().cuda(), dec_y.flatten().cuda()\n",
    "\n",
    "metric_idx = torch.pow(2,torch.arange(15))-1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "\n",
    "for lr in LR:\n",
    "\n",
    "    experiment_name='test_replay_buffers'\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
    "    log_dir = os.path.join(\n",
    "        \"runs\",experiment_name, current_time + \"_\" + socket.gethostname() \n",
    "    )\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir)\n",
    "\n",
    "    sim = SimulationEnvironment0(num_simulations=128,\n",
    "                            num_blackholes=num_blackholes, \n",
    "                            force_constant=0.002, \n",
    "                            velocity_scale=0.01,\n",
    "                            goal_threshold=0.05,\n",
    "                            max_steps=250,\n",
    "                            device='cuda')\n",
    "\n",
    "    agent = Agent((num_blackholes+2), hidden_size, levels, input_type, actor=False, value_dimension=simlog_res).cuda()\n",
    "    actor = Agent((num_blackholes+2), hidden_size, levels, input_type, critic=False, action_dimension=5**2).cuda()\n",
    "\n",
    "    optim = torch.optim.AdamW(agent.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    optim_actor = torch.optim.AdamW(actor.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "    target_agent = copy.deepcopy(agent)\n",
    "    target_actor = copy.deepcopy(actor)\n",
    "\n",
    "    replay_buffer = Replay_Buffer(state_shape=((num_blackholes+2),2), action_shape=(1,), batch_size=batch_size, size=size, device='cuda')\n",
    "\n",
    "    old_states=None\n",
    "\n",
    "    \n",
    "\n",
    "    R=[]\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    pbar = tqdm(range(training_steps))\n",
    "\n",
    "    x,y = torch.meshgrid(torch.arange(100),torch.arange(100))\n",
    "    pos = torch.stack([x.flatten(), y.flatten()],1)/100\n",
    "    target_pos = torch.ones_like(pos)*0.25\n",
    "    bh_pos = torch.ones_like(pos)*0.75\n",
    "\n",
    "    st=torch.stack([pos,target_pos,bh_pos],1)\n",
    "\n",
    "    E = []\n",
    "    VE = []\n",
    "\n",
    "\n",
    "    for i in pbar:\n",
    "        t0=time.time()\n",
    "\n",
    "        # generate experience\n",
    "        states = states.reshape(states.shape[0],-1).cuda()\n",
    "        _, values = agent(states)\n",
    "        actions, _ = actor(states)\n",
    "\n",
    "        # assert not torch.any(torch.isnan(values))\n",
    "        # assert not torch.any(torch.isinf(values.abs()))\n",
    "        # assert not torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp()))\n",
    "        # assert not torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp()))\n",
    "        if(torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp()))) or torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp())):\n",
    "            print('nan',torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp())), 'inf', torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp())))\n",
    "        # assert not torch.any(torch.isnan(actions))\n",
    "        # assert not torch.any(torch.isinf(actions.abs()))\n",
    "\n",
    "        \n",
    "        # m = actions[:,:,0]\n",
    "        # s = torch.exp(actions[:,:,1])/10\n",
    "        # print(m[0],s[0])\n",
    "\n",
    "        # sampled_action = m + torch.randn_like(m) * s\n",
    "        # action_prob = torch.exp(-torch.square((sampled_action-m)/s)/2) / (s*np.sqrt(2*np.pi))\n",
    "#        means = actions[:,:2]\n",
    "        # lower = torch.zeros(len(actions),2,2, device=actions.device)\n",
    "        # lower[:,torch.tril_indices(2,2)[0], torch.tril_indices(2,2)[1]] = actions[:,2:]\n",
    "        # lower += torch.eye(2, device='cuda')\n",
    "        # lower = (lower.abs()+1e-5).sqrt() * lower.sign()\n",
    "        # covariance = lower @ lower.transpose(-1,-2)\n",
    "#        covariance = actions[:,2:].exp().diag_embed()/2\n",
    "\n",
    "        # assert not torch.any(torch.isnan(covariance))\n",
    "        # assert not torch.any(torch.isinf(covariance.abs()))\n",
    "\n",
    "#        dist = torch.distributions.multivariate_normal.MultivariateNormal(means, covariance)\n",
    "        #dist = torch.distributions.transformed_distribution.TransformedDistribution(dist, torch.distributions.transforms.TanhTransform())\n",
    "        #H = dist.entropy()\n",
    "\n",
    "        action_probs = actions.softmax(-1)\n",
    "        H = -(action_probs*(action_probs+1e-8).log()).sum(-1)\n",
    "        # action_probs.register_hook(print)\n",
    "        dist = Categorical(action_probs)\n",
    "\n",
    "\n",
    "\n",
    "        sampled_action = dist.sample()\n",
    "        log_prob = dist.log_prob(sampled_action)\n",
    "        \n",
    "        # sampled_action_tanh = torch.tanh(sampled_action)\n",
    "        # log_prob = log_prob - torch.log( 1- torch.tanh(sampled_action).square() ).sum(-1)\n",
    "\n",
    "        # log_prob = torch.log(log_prob.exp()+1e-8)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #log_prob.register_hook(print)\n",
    "        u, v = dec_x[sampled_action], dec_y[sampled_action]\n",
    "        \n",
    "        sampled_action_decoded = torch.stack([u,v],1)\n",
    "\n",
    "        rewards, new_states, is_terminal = sim.step(sampled_action_decoded)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            _, next_values = target_agent(new_states.reshape(new_states.shape[0],-1).cuda())\n",
    "\n",
    "            expexted_next_value = (torch.softmax(next_values,1)@bin_values[:,None])[:,0]*(is_terminal.logical_not())\n",
    "\n",
    "            expecter_target_value = rewards + gamma*expexted_next_value\n",
    "\n",
    "\n",
    "            # actions_t, _ = target_actor(states)\n",
    "            # means_t = actions_t[:,:2]\n",
    "            # covariance_t = actions_t[:,2:].exp().diag_embed()\n",
    "            # dist_t = torch.distributions.multivariate_normal.MultivariateNormal(means_t, covariance_t)\n",
    "            # log_prob_t = dist_t.log_prob(sampled_action)\n",
    "            # log_prob_t = log_prob_t - torch.log( 1- torch.tanh(sampled_action_tanh).square() ).sum(-1)\n",
    "\n",
    "        \n",
    "        # assert not torch.any(torch.isnan(expecter_target_value))\n",
    "        # assert not torch.any(torch.isinf(expecter_target_value.abs()))\n",
    "\n",
    "        y = two_hot_encode(expecter_target_value, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "        \n",
    "        # assert not torch.any(torch.isnan(y))\n",
    "        # assert not torch.any(torch.isinf(y.abs()))\n",
    "        \n",
    "        critic_error = torch.nn.functional.cross_entropy(values, y, reduction='none')\n",
    "        \n",
    "        # assert not torch.any(torch.isnan(critic_error))\n",
    "        # assert not torch.any(torch.isinf(critic_error.abs()))\n",
    "\n",
    "        expexted_value = (torch.softmax(values,1)@bin_values[:,None])[:,0]\n",
    "        expected_prediction_error = (expexted_value - expecter_target_value)\n",
    "        actor_error = expected_prediction_error.detach() * log_prob - H*0.001 + 0.01*action_probs.square().mean(-1)#+ log_prob * 0.1 #- H * 1e-2\n",
    "        \n",
    "        assert not torch.any(torch.isnan(actor_error))\n",
    "        assert not torch.any(torch.isinf(actor_error.abs()))\n",
    "\n",
    "        #r = log_prob.exp()/(log_prob_t.exp()+1e-8)\n",
    "        #actor_error = torch.maximum( expected_prediction_error.detach() * r, expected_prediction_error.detach() * r.clip(1-eps,1+eps)) + log_prob * 0. + (log_prob - log_prob_t).square()*1e-5\n",
    "\n",
    "\n",
    "        error =  critic_error #+ actor_error*10\n",
    "\n",
    "        optim.zero_grad()\n",
    "        error.mean().backward()\n",
    "        optim.step()\n",
    "\n",
    "        optim_actor.zero_grad()\n",
    "        actor_error.mean().backward()\n",
    "\n",
    "        for parameter in actor.parameters():\n",
    "            assert not torch.any(torch.isnan(parameter.grad))\n",
    "            assert not torch.any(torch.isinf(parameter.grad.abs()))\n",
    "\n",
    "        optim_actor.step()\n",
    "\n",
    "        if isinstance(replay_buffer, Replay_Buffer):\n",
    "            replay_buffer.add_experience(states=states, actions=sampled_action, action_probs=log_prob, rewards=rewards, next_states=new_states, terminals=is_terminal)\n",
    "        elif isinstance(replay_buffer, Prioritized_Replay_Buffer):\n",
    "            replay_buffer.add_experience(states=states, actions=sampled_action, action_probs=log_prob, rewards=rewards, next_states=new_states, terminals=is_terminal, weights=critic_error+1e-7) # \n",
    "\n",
    "        E.append(critic_error.mean().item())\n",
    "\n",
    "        tb_writer.add_scalar('TD error', expected_prediction_error.mean().item(), i)\n",
    "        tb_writer.add_scalar('Actor error', actor_error.mean().item(), i)\n",
    "        tb_writer.add_scalar('Critic error', critic_error.mean().item(), i)\n",
    "        tb_writer.add_scalar('Reward',rewards.mean().item(), i)\n",
    "\n",
    "        # train actor with PPO\n",
    "        old_probs = log_prob.exp().detach()\n",
    "        for b_idx in range(0):\n",
    "\n",
    "            actions, _ = actor(states)\n",
    "            # assert not torch.any(torch.isnan(actions))\n",
    "            # assert not torch.any(torch.isinf(actions.abs()))\n",
    "#            means = actions[:,:2]\n",
    "#            covariance = actions[:,2:].exp().diag_embed()/2\n",
    "            # assert not torch.any(torch.isnan(covariance))\n",
    "            # assert not torch.any(torch.isinf(covariance.abs()))\n",
    "#            dist = torch.distributions.multivariate_normal.MultivariateNormal(means, covariance)\n",
    "\n",
    "            action_probs = actions.softmax(-1)\n",
    "            H = -(action_probs*(action_probs+1e-8).log()).sum(-1)\n",
    "            # action_probs.register_hook(print)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "            #sampled_action = dist.sample()\n",
    "            log_prob = dist.log_prob(sampled_action)\n",
    "            \n",
    "            #sampled_action = torch.tanh(sampled_action)\n",
    "#            log_prob = log_prob - torch.log( 1- torch.tanh(sampled_action).square() ).sum(-1)\n",
    "\n",
    "            probs = log_prob.exp()\n",
    "\n",
    "            #r = probs/old_probs.detach()\n",
    "            r = log_prob.exp()/(old_probs+1e-8)\n",
    "\n",
    "            # other_points = torch.rand(16,128,2).cuda()*4-2\n",
    "            # other_logprobs = dist.log_prob(other_points)\n",
    "            # other_points_tanh = torch.tanh(other_points)\n",
    "            # other_logprobs = other_logprobs - torch.log( 1- torch.tanh(other_points_tanh).square() ).sum(-1)\n",
    "            #H = -( other_logprobs*other_logprobs.exp() ).mean(0)/2 - torch.log(log_prob.exp()+1e-8)/2 \n",
    "            #    - H*0.001\n",
    "\n",
    "            actor_error = torch.maximum( expected_prediction_error.detach() * r, expected_prediction_error.detach() * r.clip(1-eps,1+eps)) - H*0.01 # + (log_prob - log_prob_t).square()*0.001\n",
    "\n",
    "            optim_actor.zero_grad()\n",
    "            actor_error.mean().backward()\n",
    "            optim_actor.step()\n",
    "\n",
    "            \n",
    "        # if (i+1)%16==0:\n",
    "        #     for e in range(epochs):\n",
    "        #         optim_actor.zero_grad()\n",
    "        #         for b_idx in range(len(replay_buffer)):\n",
    "\n",
    "        #             b_states, b_actions, b_log_prob, b_rewards, b_next_states, b_terminals = replay_buffer.get(b_idx) #, weights get_high_priority_batch\n",
    "\n",
    "        #             _, values = agent(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "\n",
    "                    \n",
    "\n",
    "        #             actions, _ = actor(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "\n",
    "        #             with torch.inference_mode():\n",
    "        #                 _, next_values = target_agent(b_next_states.reshape(b_next_states.shape[0],-1).cuda())\n",
    "\n",
    "        #                 expexted_next_value = (torch.softmax(next_values,1)@bin_values[:,None])[:,0]*(b_terminals.logical_not())\n",
    "\n",
    "        #                 expecter_target_value = b_rewards + gamma*expexted_next_value\n",
    "\n",
    "                                    \n",
    "        #             y = two_hot_encode(expecter_target_value, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "\n",
    "        #             critic_error = (torch.nn.functional.cross_entropy(values, y, reduction='none')) #/ weights off_policy_coeff.detach()*\n",
    "\n",
    "        #             optim.zero_grad()\n",
    "        #             critic_error.mean().backward()\n",
    "        #             optim.step()\n",
    "                    \n",
    "\n",
    "        #             expexted_value = (torch.softmax(values,1)@bin_values[:,None])[:,0]\n",
    "        #             expected_prediction_error = (expexted_value - expecter_target_value)\n",
    "\n",
    "        #             # assert not torch.any(torch.isnan(actions))\n",
    "        #             # assert not torch.any(torch.isinf(actions.abs()))\n",
    "        #             means = actions[:,:2]\n",
    "        #             covariance = actions[:,2:].exp().diag_embed()/2\n",
    "        #             # assert not torch.any(torch.isnan(covariance))\n",
    "        #             # assert not torch.any(torch.isinf(covariance.abs()))\n",
    "        #             dist = torch.distributions.multivariate_normal.MultivariateNormal(means, covariance)\n",
    "\n",
    "        #             #sampled_action = dist.sample()\n",
    "        #             log_prob = dist.log_prob(b_actions)\n",
    "                    \n",
    "        #             #sampled_action = torch.tanh(sampled_action)\n",
    "        #             log_prob = log_prob - torch.log( 1- torch.tanh(b_actions).square() ).sum(-1)\n",
    "\n",
    "        #             #r = probs/old_probs.detach()\n",
    "        #             r = log_prob.exp()/(b_log_prob.exp()+1e-8)\n",
    "\n",
    "        #             # other_points = torch.rand(16,128,2).cuda()*4-2\n",
    "        #             # other_logprobs = dist.log_prob(other_points)\n",
    "        #             # other_points_tanh = torch.tanh(other_points)\n",
    "        #             # other_logprobs = other_logprobs - torch.log( 1- torch.tanh(other_points_tanh).square() ).sum(-1)\n",
    "        #             #H = -( other_logprobs*other_logprobs.exp() ).mean(0)/2 - torch.log(log_prob.exp()+1e-8)/2 \n",
    "        #             #    - H*0.001\n",
    "\n",
    "        #             actor_error = torch.maximum( expected_prediction_error.detach() * r, expected_prediction_error.detach() * r.clip(1-eps,1+eps))# + (log_prob - log_prob_t).square()*0.001\n",
    "\n",
    "        #             actor_error.mean().backward()\n",
    "        #         optim_actor.step()\n",
    "\n",
    "\n",
    "        update_target_model(model=agent, target_model=target_agent, decay=1e-3)\n",
    "        update_target_model(model=actor, target_model=target_actor, decay=1e-4)\n",
    "\n",
    "\n",
    "        # train critic on old experience\n",
    "        for b_idx in range(0):\n",
    "            b_states, b_actions, b_log_prob, b_rewards, b_next_states, b_terminals, weights = replay_buffer.get_batch() #, weights get_high_priority_batch\n",
    "\n",
    "            _, values = agent(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "\n",
    "            # assert not torch.any(torch.isnan(values))\n",
    "            # assert not torch.any(torch.isinf(values.abs()))\n",
    "            # assert not torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp()))\n",
    "            # assert not torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp()))\n",
    "            if(torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp()))) or torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp())):\n",
    "                print('nan',torch.any(torch.isnan((values-values.max(1)[0][:,None]).exp())), 'inf', torch.any(torch.isinf((values-values.max(1)[0][:,None]).exp())))\n",
    "\n",
    "            #actions, _ = actor(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "\n",
    "            means = actions[:,:2]\n",
    "            covariance = actions[:,2:].exp().diag_embed()\n",
    "            dist = torch.distributions.multivariate_normal.MultivariateNormal(means, covariance)\n",
    "\n",
    "            # lower = torch.zeros(len(actions),2,2, device=actions.device)\n",
    "            # lower[:,torch.tril_indices(2,2)[0], torch.tril_indices(2,2)[1]] = actions[:,2:]\n",
    "            # lower += torch.eye(2, device='cuda')\n",
    "            # lower = (lower.abs()+1e-5).sqrt() * lower.sign()\n",
    "            # covariance = lower @ lower.transpose(-1,-2) \n",
    "            \n",
    "\n",
    "            # sampled_action = dist.sample()\n",
    "            # sampled_action = torch.tanh(sampled_action)\n",
    "\n",
    "            # log_prob = dist.log_prob(b_actions)\n",
    "            # log_prob = log_prob - torch.log( 1- torch.tanh(b_actions).square() ).sum(-1)\n",
    "\n",
    "            # actions = actions.reshape(actions.shape[0],2,-1)\n",
    "            # actions/=2\n",
    "            # action_probs = actions.softmax(-1)\n",
    "            # action_prob = action_probs.gather(-1, ((b_actions+1)/2*5).long()[:,:,None]).prod(1).squeeze(1)\n",
    "            # # assert not torch.any(torch.isnan(action_prob))\n",
    "            \n",
    "            # off_policy_coeff = torch.exp((log_prob - b_log_prob).clip(-100,5))\n",
    "            # off_policy_coeff/=off_policy_coeff.sum()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                _, next_values = target_agent(b_next_states.reshape(b_next_states.shape[0],-1).cuda())\n",
    "\n",
    "                expexted_next_value = (torch.softmax(next_values,1)@bin_values[:,None])[:,0]*(b_terminals.logical_not())\n",
    "\n",
    "                expecter_target_value = b_rewards + gamma*expexted_next_value\n",
    "\n",
    "            \n",
    "            # assert not torch.any(torch.isnan(expecter_target_value))\n",
    "            # assert not torch.any(torch.isinf(expecter_target_value.abs()))\n",
    "\n",
    "            y = two_hot_encode(expecter_target_value, simlog_max_range, simlog_res, simlog_half_res, smoothing=smoothing)\n",
    "            \n",
    "            # assert not torch.any(torch.isnan(y))\n",
    "            # assert not torch.any(torch.isinf(y.abs()))\n",
    "            \n",
    "            critic_error = (torch.nn.functional.cross_entropy(values, y, reduction='none')) #/ weights off_policy_coeff.detach()*\n",
    "\n",
    "            # assert not torch.any(torch.isnan(critic_error))\n",
    "            # assert not torch.any(torch.isinf(critic_error.abs()))\n",
    "\n",
    "            expexted_value = (torch.softmax(values,1)@bin_values[:,None])[:,0]\n",
    "            expected_prediction_error = (expexted_value - expecter_target_value)\n",
    "            #actor_error = (expected_prediction_error.detach() * log_prob - log_prob * 1e-2) #- H * 1e-2 off_policy_coeff.detach()*\n",
    "\n",
    "            if weights is not None:\n",
    "                critic_error /= weights\n",
    "                #actor_error /= weights\n",
    "\n",
    "            # actor_error = off_policy_coeff.detach() * expected_prediction_error.detach() * torch.log(action_prob)\n",
    "\n",
    "            error =  critic_error #+ actor_error*0.01\n",
    "\n",
    "            optim.zero_grad()\n",
    "            error.mean().backward()\n",
    "            optim.step()\n",
    "\n",
    "            \n",
    "            # with torch.inference_mode():\n",
    "            #     actions_t, _ = target_actor(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "            #     means_t = actions_t[:,:2]\n",
    "            #     covariance_t = actions_t[:,2:].exp().diag_embed()\n",
    "            #     dist_t = torch.distributions.multivariate_normal.MultivariateNormal(means_t, covariance_t)\n",
    "            #     log_prob_t = dist_t.log_prob(b_actions)\n",
    "            #     log_prob_t = log_prob_t - torch.log( 1- torch.tanh(b_actions.tanh()).square() ).sum(-1)\n",
    "\n",
    "            # actions, _ = actor(b_states.reshape(b_states.shape[0],-1).cuda())\n",
    "            # means = actions[:,:2]\n",
    "            # covariance = actions[:,2:].exp().diag_embed()/2\n",
    "            # dist = torch.distributions.multivariate_normal.MultivariateNormal(means, covariance)\n",
    "            # log_prob = dist.log_prob(b_actions)\n",
    "            # log_prob = log_prob - torch.log( 1- torch.tanh(b_actions.tanh()).square() ).sum(-1)\n",
    "\n",
    "            # probs = log_prob.exp()\n",
    "\n",
    "            # #r = probs/old_probs.detach()\n",
    "            # r = log_prob.exp()/(log_prob_t.exp()+1e-8)\n",
    "\n",
    "            # # other_points = torch.rand(16,128,2).cuda()*4-2\n",
    "            # # other_logprobs = dist.log_prob(other_points)\n",
    "            # # other_points_tanh = torch.tanh(other_points)\n",
    "            # # other_logprobs = other_logprobs - torch.log( 1- torch.tanh(other_points_tanh).square() ).sum(-1)\n",
    "            # #H = -( other_logprobs*other_logprobs.exp() ).mean(0)/2 - torch.log(log_prob.exp()+1e-8)/2 \n",
    "            # #    - H*0.001\n",
    "\n",
    "            # actor_error = torch.maximum( expected_prediction_error.detach() * r, expected_prediction_error.detach() * r.clip(1-eps,1+eps)) + (log_prob - log_prob_t).square()*0.001\n",
    "\n",
    "            # optim_actor.zero_grad()\n",
    "            # actor_error.mean().backward()\n",
    "            # optim_actor.step()\n",
    "\n",
    "            # optim_actor.zero_grad()\n",
    "            # actor_error.mean().backward()\n",
    "            # optim_actor.step()\n",
    "\n",
    "            if weights is not None:\n",
    "                replay_buffer.update_weights(critic_error+1e-7)\n",
    "\n",
    "\n",
    "            update_target_model(model=agent, target_model=target_agent, decay=1e-3)\n",
    "\n",
    "        states = new_states\n",
    "\n",
    "        if i % 8 == 0:\n",
    "            pbar.set_postfix_str(#f'{pred_error.mean().item():.3g}'.ljust(simlog_max_range)+\n",
    "                                #f'{actor_error.mean().item():.3g}'.ljust(simlog_max_range)+\n",
    "                                f'{error.mean().item():.3g}'.ljust(10)+\n",
    "                                f'{rewards.mean().item():.3g}'.ljust(10))\n",
    "\n",
    "        R.append(rewards.mean().item())\n",
    "\n",
    "\n",
    "        \n",
    "        if (i+1) % validate_every == 0:\n",
    "\n",
    "            V = []\n",
    "            A = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v = agent(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                a, _ = actor(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                V.append(v)\n",
    "                A.append(a)\n",
    "            V = torch.concat(V,0)\n",
    "            A = torch.concat(A,0)\n",
    "\n",
    "            V_t = []\n",
    "            for b in range((len(st)+batch_size-1)//batch_size):\n",
    "                stb = st[b*batch_size:(b+1)*batch_size]\n",
    "                _, v1 = target_agent(stb.reshape(stb.shape[0],-1).cuda())\n",
    "                #_, v2 = agent2(st.reshape(st.shape[0],-1).cuda())\n",
    "                V_t.append(v1)\n",
    "            V_t = torch.concat(V_t,0)\n",
    "            \n",
    "            \n",
    "            #plt.plot(V[0].detach().cpu())\n",
    "            # V = (V.softmax(1).detach().cpu() * torch.arange(simlog_res)).sum(1) - simlog_half_res\n",
    "            # V = V/simlog_half_res*simlog_max_range\n",
    "            # V = (V.abs().exp()-1)*V.sign()\n",
    "            V = (V.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "\n",
    "            # V_t = (V_t.softmax(1).detach().cpu() * torch.arange(simlog_res)).sum(1) - simlog_half_res\n",
    "            # V_t = V_t/simlog_half_res*simlog_max_range\n",
    "            # V_t = (V_t.abs().exp()-1)*V_t.sign()\n",
    "            V_t = (V_t.softmax(1)@bin_values[:,None])[:,0].detach().cpu()\n",
    "            \n",
    "            #plt.show()\n",
    "            #print(V.mean())\n",
    "            #input()\n",
    "\n",
    "            # clear_output()\n",
    "\n",
    "            # print(V.mean())\n",
    "            # fig = plt.figure(figsize=(15,16))\n",
    "            # gs = GridSpec(8, 6, figure=fig)\n",
    "\n",
    "            # plt.subplot(gs[:2,:2])\n",
    "            # plt.imshow(V.reshape(100,100),vmin=-1, vmax=1)\n",
    "            tb_writer.add_image('V', (V.reshape(1,100,100)/2+0.5), i)\n",
    "\n",
    "            # plt.subplot(gs[:2,2:4])\n",
    "            # plt.imshow(V_t.reshape(100,100),vmin=-1, vmax=1)\n",
    "            # plt.colorbar()\n",
    "            tb_writer.add_image('V_t', V_t.reshape(1,100,100)/2+0.5, i)\n",
    "\n",
    "            # plt.subplot(gs[:2,4:])\n",
    "            # plt.imshow((torch.tensor(value_resized)-V.reshape(100,100)).square())\n",
    "            # plt.colorbar()\n",
    "            #tb_writer.add_image('Validation Value Error', (value_resized-V.reshape(1,100,100)).square().clip(0,1), i)\n",
    "\n",
    "            # plt.subplot(gs[4:,1:5])\n",
    "\n",
    "            pos = st[:,0].reshape(100,100,-1)[::2,::2]\n",
    "            A = A.reshape(100,100,-1)[::2,::2]\n",
    "            # A = A/2\n",
    "            action_probs = A.softmax(-1)\n",
    "            u = action_probs@dec_x[:,None]\n",
    "            v = action_probs@dec_y[:,None]\n",
    "            A = torch.concat([u,v],-1)\n",
    "            \n",
    "            plt.quiver(pos[...,1].flatten(), -pos[...,0].flatten(), A[...,1].tanh().detach().cpu().flatten(), -A[...,0].tanh().detach().cpu().flatten(), color='g',scale=50, headwidth=2)\n",
    "            ax.axis('off')\n",
    "            plt.gca().set_aspect('equal')\n",
    "            plt.subplots_adjust(0,0,1,1,0,0)\n",
    "            fig.canvas.draw()\n",
    "            data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "            tb_writer.add_image('Policy visualization', np.transpose(data,(2,0,1)) , i)\n",
    "            plt.clf()\n",
    "            #plt.show()\n",
    "\n",
    "            # plt.subplot(gs[2:4,:2])\n",
    "            # plt.plot(E)\n",
    "            # plt.plot(np.convolve(np.array(E), np.ones(100),'same')/np.convolve(np.ones(len(E)), np.ones(100),'same'))\n",
    "            # plt.yscale('log')\n",
    "            \n",
    "\n",
    "            \n",
    "            # plt.subplot(gs[2:4,2:4])\n",
    "            # plt.plot(R)\n",
    "            # plt.plot(np.convolve(np.array(R), np.ones(100),'same')/np.convolve(np.ones(len(R)), np.ones(100),'same'))\n",
    "            \n",
    "            # plt.subplot(gs[2:4,4:])\n",
    "            val_err = (value_resized-V.reshape(1,100,100)).square().mean().item()\n",
    "            VE.append(val_err)\n",
    "            # plt.plot(VE)\n",
    "            #plt.show()\n",
    "            tb_writer.add_scalar('Validation Error',val_err, i)\n",
    "\n",
    "        #tb_writer.add_scalar('Iteration time',time.time()-t0, i)\n",
    "    \n",
    "    indices = metric_idx[metric_idx<len(VE)]\n",
    "    tb_writer.add_hparams(  {\"lr\":lr,\n",
    "                            \"epochs\":epochs,\n",
    "                            \"gamma\": gamma,\n",
    "                            \"smoothing\":smoothing,\n",
    "                             },\n",
    "                            {f\"error_at_{int((indices[i]+1)*validate_every)}\": ve.item() for i,ve in enumerate(torch.tensor(VE)[indices])})\n",
    "    \n",
    "\n",
    "    torch.save({\n",
    "    \"E\":torch.tensor(E),\n",
    "    \"VE\":torch.tensor(VE),\n",
    "    },\n",
    "    os.path.join(log_dir,'results.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 23.4095,  12.3554,  33.1925,  25.2574,  72.6465,   5.9458,   2.1978,\n",
       "         19.1969,  33.1635,  25.4218, -12.5538,   0.2201,  10.0015,  24.0330,\n",
       "         30.6597, -18.6875, -15.5374,  10.9901,   7.9407,  20.0727, -35.9163,\n",
       "        -13.3782,  -9.1114,   5.9383,  14.0954], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.1366e-22, 6.5453e-27, 7.3345e-18, 2.6252e-21, 1.0000e+00, 1.0772e-29,\n",
       "        2.5382e-31, 6.1254e-24, 7.1248e-18, 3.0945e-21, 9.9536e-38, 3.5127e-32,\n",
       "        6.2178e-28, 7.7165e-22, 5.8258e-19, 2.1584e-40, 5.0377e-39, 1.6710e-27,\n",
       "        7.9184e-29, 1.4707e-23, 0.0000e+00, 4.3647e-38, 3.1117e-36, 1.0690e-29,\n",
       "        3.7289e-26], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[-3].softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0367e-20, -3.9462e-25, -2.8938e-16, -1.2441e-19,  0.0000e+00,\n",
       "        -7.1847e-28, -1.7881e-29, -3.2740e-22, -2.8131e-16, -1.4614e-19,\n",
       "        -8.4805e-36, -2.5441e-30, -3.8951e-26, -3.7513e-20, -2.4461e-17,\n",
       "        -1.9714e-38, -4.4424e-37, -1.0303e-25, -5.1237e-27, -7.7319e-22,\n",
       "                nan, -3.7547e-36, -2.5440e-34, -7.1313e-28, -2.1833e-24],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs[-3]*action_probs[-3].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0472, 0.0428, 0.8490, 0.9992, 0.0380, 0.0313, 0.0405, 0.0374, 0.9998,\n",
       "        0.0341, 0.0621, 0.9937, 0.0402, 0.0434, 0.0371, 0.0443, 0.0421, 0.0293,\n",
       "        0.9920, 0.0387, 0.0429, 0.0481, 0.0431, 0.9970, 0.0365, 0.0413, 0.0451,\n",
       "        0.0419, 0.0427, 0.0404, 0.0395, 0.1285, 0.9669, 0.0497, 0.0360, 0.0440,\n",
       "        0.0347, 0.0334, 0.0421, 0.0331, 0.5921, 0.0456, 0.7926, 0.0416, 0.9987,\n",
       "        0.0368, 0.9901, 0.8634, 0.0373, 1.0000, 0.0396, 0.0457, 0.0393, 0.0379,\n",
       "        0.0440, 0.0354, 0.0576, 0.0416, 0.9999, 0.0359, 0.0329, 0.0419, 0.0407,\n",
       "        0.0398, 0.0473, 0.0420, 0.0397, 0.0414, 0.0485, 0.0404, 0.9208, 0.0374,\n",
       "        0.0458, 0.0466, 0.0467, 0.0402, 0.0540, 0.0458, 0.0451, 0.0386, 0.2877,\n",
       "        0.0401, 0.0475, 1.0000, 0.0416, 0.0476, 0.0389, 0.0445, 0.0452, 0.0480,\n",
       "        0.0475, 0.0447, 0.0477, 0.0395, 0.0495, 0.0353, 0.0385, 0.0458, 0.0365,\n",
       "        1.0000, 0.9640, 0.0360, 0.0357, 1.0000, 0.0343, 0.0523, 0.0341, 0.9988,\n",
       "        0.0376, 0.9999, 0.0357, 0.0382, 0.0347, 0.0390, 0.0375, 0.0488, 0.0397,\n",
       "        0.0309, 0.0590, 0.0487, 0.0350, 0.0610, 0.0339, 0.0417, 0.0452, 1.0000,\n",
       "        0.0471, 0.0439], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
